{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1aa6f3d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cloning into 'PODFCSSV'...\n",
                        "remote: Enumerating objects: 261, done.\u001b[K\n",
                        "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
                        "remote: Total 261 (delta 0), reused 2 (delta 0), pack-reused 258 (from 1)\u001b[K\n",
                        "Receiving objects: 100% (261/261), 4.00 MiB | 18.97 MiB/s, done.\n",
                        "Resolving deltas: 100% (112/112), done.\n"
                    ]
                }
            ],
            "source": [
                "# import os\n",
                "# import shutil\n",
                "\n",
                "# for item in os.listdir('.'):\n",
                "#     if os.path.isfile(item) or os.path.islink(item):\n",
                "#         os.remove(item)\n",
                "#     elif os.path.isdir(item):\n",
                "#         shutil.rmtree(item)\n",
                "!git clone https://github.com/sathishkumar67/PODFCSSV.git\n",
                "!mv PODFCSSV/* /kaggle/working"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "83851c81",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Main] Loading pre-trained ViTMAE...\n",
                        "\n",
                        "============================================================\n",
                        "[System] Starting Adapter Injection Procedure\n",
                        "============================================================\n",
                        "[Config] Freezing original backbone parameters...\n",
                        "[Config] Model Config: Hidden Dim=768, Layers=12\n",
                        "[Config] Adapter Config: Bottleneck Dim=64\n",
                        "[Action] Injecting adapters into encoder layers...\n",
                        "  -> Processed layer 4/12\n",
                        "  -> Processed layer 8/12\n",
                        "  -> Processed layer 12/12\n",
                        "[System] Injection Complete. Decoder layers ignored (if present).\n",
                        "\n",
                        "[Stats] Parameter Audit:\n",
                        "  - Total Parameters:     113,097,472\n",
                        "  - Frozen Backbone:      111,907,840\n",
                        "  - Trainable (Adapters): 1,189,632\n",
                        "  - Trainable Ratio:      1.05%\n",
                        "============================================================\n",
                        "\n",
                        "[Main] Running dummy forward pass to verify graph integrity...\n",
                        "[Success] Standard Forward pass complete. Loss: 0.998232364654541\n"
                    ]
                }
            ],
            "source": [
                "from __future__ import annotations\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from typing import Optional, Tuple, Union, List, Any\n",
                "from transformers import PreTrainedModel, ViTMAEForPreTraining\n",
                "\n",
                "class IBA_Adapter(nn.Module):\n",
                "    \"\"\"\n",
                "    Information-Bottlenecked Adapter (IBA) Module for Efficient parameter-efficient Fine-Tuning.\n",
                "\n",
                "    Overview\n",
                "    --------\n",
                "    The IBA Adapter is a lightweight neural network module designed to be inserted \n",
                "    into pre-trained frozen backbones (like ViT or BERT). It introduces a small \n",
                "    number of trainable parameters to adapt the model to new tasks (or domains) \n",
                "    without retraining the entire massive network.\n",
                "\n",
                "    Architectural Design\n",
                "    --------------------\n",
                "    The adapter follows a \"Bottleneck\" structure to minimize parameter count while \n",
                "    maximizing adaptation capability. Ideally, it compresses high-dimensional \n",
                "    semantic features into a compact representation and then reconstructs them.\n",
                "\n",
                "    Structure:\n",
                "        Input (D) -> Down-Projection (d) -> Non-Linearity -> Up-Projection (D) -> Dropout -> + Residual\n",
                "\n",
                "    Key Design Principles\n",
                "    ---------------------\n",
                "    1.  **Information Bottleneck**: By projecting high-dimensional features (D) \n",
                "        down to a smaller dimension (d), the model is forced to learn only the \n",
                "        most salient features relevant to the specific task, ignoring noise.\n",
                "    \n",
                "    2.  **Identity Initialization**: A critical stability feature for Federated Learning.\n",
                "        -   The Up-Projection layer is initialized with **zeros**.\n",
                "        -   This ensures that at initialization (step 0), the adapter output is exactly 0.\n",
                "        -   Result: `Layer(x) + Adapter(x) = Layer(x) + 0 = Layer(x)`.\n",
                "        -   This prevents \"catastrophic forgetting\" or \"semantic shock\" where random \n",
                "            initialization would distort the carefully learned features of the \n",
                "            pre-trained backbone.\n",
                "\n",
                "    Attributes\n",
                "    ----------\n",
                "    input_dim : int\n",
                "        The dimensionality of the input features (Hidden Size of the backbone).\n",
                "    bottleneck_dim : int\n",
                "        The dimensionality of the compressed bottleneck space.\n",
                "    down_project : nn.Linear\n",
                "        Linear layer reducing dimension from `input_dim` to `bottleneck_dim`.\n",
                "    activation : nn.Module\n",
                "        Non-linear activation function (e.g., GELU, ReLU) to enable learning complex patterns.\n",
                "    up_project : nn.Linear\n",
                "        Linear layer restoring dimension from `bottleneck_dim` back to `input_dim`.\n",
                "    dropout : nn.Dropout\n",
                "        Dropout layer for regularization during training.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self, \n",
                "        input_dim: int, \n",
                "        bottleneck_dim: int = 64, \n",
                "        dropout: float = 0.0,\n",
                "        activation: nn.Module = nn.GELU()\n",
                "    ) -> None:\n",
                "        \"\"\"\n",
                "        Initializes the IBA Adapter with the specified configuration.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        input_dim : int\n",
                "            The hidden size of the pre-trained model (e.g., 768 for ViT-Base).\n",
                "        bottleneck_dim : int, optional\n",
                "            The size of the bottleneck. Smaller values result in fewer parameters \n",
                "            but may limit capacity. Defaults to 64.\n",
                "        dropout : float, optional\n",
                "            Dropout probability applied to the output of the adapter. Defaults to 0.0.\n",
                "        activation : nn.Module, optional\n",
                "            The activation function to use within the bottleneck. Defaults to nn.GELU().\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.input_dim = input_dim\n",
                "        self.bottleneck_dim = bottleneck_dim\n",
                "        self.activation = activation\n",
                "\n",
                "        # 1. Down-Projection Layer\n",
                "        # Compresses the input semantic vector into the bottleneck space (D -> d).\n",
                "        self.down_project = nn.Linear(input_dim, bottleneck_dim, bias=True)\n",
                "        \n",
                "        # 2. Up-Projection Layer\n",
                "        # Reconstructs the semantic vector from the bottleneck space (d -> D).\n",
                "        self.up_project = nn.Linear(bottleneck_dim, input_dim, bias=True)\n",
                "        \n",
                "        # 3. Regularization\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        # 4. Weight Initialization\n",
                "        # Apply strict initialization rules to ensure stable convergence.\n",
                "        self._init_weights()\n",
                "\n",
                "    def _init_weights(self) -> None:\n",
                "        \"\"\"\n",
                "        Applies robust initialization strategies for the adapter layers.\n",
                "\n",
                "        Initialization Strategy\n",
                "        -----------------------\n",
                "        1.  **Down-Projection**: \n",
                "            -   **Weights**: Kaiming Normal (He Initialization) with 'relu' nonlinearity mode. \n",
                "                This maintains the variance of activations through the layer, preventing \n",
                "                vanishing/exploding gradients in the bottleneck.\n",
                "            -   **Bias**: Initialized to Zero.\n",
                "\n",
                "        2.  **Up-Projection**:\n",
                "            -   **Weights & Bias**: Zero Initialization. \n",
                "            -   **Reasoning**: This ensures the adapter contributes nothing (0) at the \n",
                "                very start of training. The model initially behaves exactly like the \n",
                "                original frozen backbone, allowing the adapter to gradually learn \n",
                "                modifications rather than starting with random noise.\n",
                "        \"\"\"\n",
                "        # A. Down-Projection Initialization\n",
                "        # We use 'mode=fan_out' and 'nonlinearity=relu' as a robust default for linear layers followed by activations.\n",
                "        nn.init.kaiming_normal_(self.down_project.weight, mode='fan_out', nonlinearity='relu')\n",
                "        if self.down_project.bias is not None:\n",
                "            nn.init.zeros_(self.down_project.bias)\n",
                "        \n",
                "        # B. Up-Projection Initialization (Identity Init)\n",
                "        # This is the most critical step for stability in fine-tuning.\n",
                "        nn.init.zeros_(self.up_project.weight)\n",
                "        if self.up_project.bias is not None:\n",
                "            nn.init.zeros_(self.up_project.bias)\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Executes the forward pass of the adapter module.\n",
                "\n",
                "        The flow is:\n",
                "        Input -> [Down Project] -> [Activation] -> [Up Project] -> [Dropout] -> + Input (Residual)\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        x : torch.Tensor\n",
                "            The input hidden states from the transformer block.\n",
                "            Shape: [Batch_Size, Sequence_Length, Hidden_Dimension]\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        torch.Tensor\n",
                "            The adapted hidden states, with the exact same shape as the input.\n",
                "        \"\"\"\n",
                "        # Save the original input for the residual connection\n",
                "        residual = x\n",
                "        \n",
                "        # 1. Compression: Project down to bottleneck dimension\n",
                "        x = self.down_project(x)\n",
                "        \n",
                "        # 2. Non-Linearity: Apply activation function\n",
                "        x = self.activation(x)\n",
                "        \n",
                "        # 3. Reconstruction: Project back up to original dimension\n",
                "        x = self.up_project(x)\n",
                "        \n",
                "        # 4. Regularization: Apply dropout\n",
                "        x = self.dropout(x)\n",
                "        \n",
                "        # 5. Residual Connection: Add the learned delta to the original features\n",
                "        return residual + x\n",
                "\n",
                "    def __repr__(self) -> str:\n",
                "        \"\"\"\n",
                "        Returns a string representation of the module for debugging purposes.\n",
                "        \"\"\"\n",
                "        return f\"IBA_Adapter(in_features={self.input_dim}, bottleneck={self.bottleneck_dim})\"\n",
                "\n",
                "\n",
                "class ViTBlockWithAdapter(nn.Module):\n",
                "    \"\"\"\n",
                "    Wrapper Module to Inject an Adapter into a Frozen Transformer Block.\n",
                "\n",
                "    Purpose\n",
                "    -------\n",
                "    This class wraps an existing (frozen) `ViTLayer` or `BertLayer` from the \n",
                "    Hugging Face library. It intercepts the forward pass, allows the original \n",
                "    block to process the input, and then applies the `IBA_Adapter` to the output \n",
                "    hidden states.\n",
                "\n",
                "    It ensures compatibility with Hugging Face's complex return types \n",
                "    (tuples vs ModelOutput objects) so that the rest of the model pipeline \n",
                "    remains unaware of the modification.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, original_block: nn.Module, adapter: IBA_Adapter) -> None:\n",
                "        \"\"\"\n",
                "        Wraps a transformer block with an adapter.\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        original_block : nn.Module\n",
                "            The original, frozen Transformer block (e.g., `ViTLayer`).\n",
                "        adapter : IBA_Adapter\n",
                "            The trainable adapter instance to be applied after the block.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.original_block = original_block\n",
                "        self.adapter = adapter\n",
                "\n",
                "    def forward(\n",
                "        self, \n",
                "        hidden_states: torch.Tensor,\n",
                "        *args,\n",
                "        **kwargs\n",
                "    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, Any]]:\n",
                "        \"\"\"\n",
                "        Forward pass that mimics the signature of a standard Hugging Face ViTLayer.\n",
                "        \n",
                "        Note: arguments like `head_mask` or `output_attentions` are implicitly handled \n",
                "        or omitted based on the specific requirements of the backbone (e.g., ViTMAE \n",
                "        does not support `head_mask`).\n",
                "\n",
                "        Parameters\n",
                "        ----------\n",
                "        hidden_states : torch.Tensor\n",
                "            Input tensor of shape [Batch, SeqLen, Dim].\n",
                "        args : tuple\n",
                "            Variable positional arguments required by the pipeline.\n",
                "        kwargs : dict\n",
                "            Variable keyword arguments required by the pipeline.\n",
                "\n",
                "        Returns\n",
                "        -------\n",
                "        Union[Tuple[torch.Tensor], Tuple[torch.Tensor, Any]]\n",
                "            The output tuple expected by the transformer model, containing the \n",
                "            adapted hidden states and potentially attention weights.\n",
                "        \"\"\"\n",
                "        # 1. Execute the Original Frozen Block\n",
                "        # We explicitly ignored *args and **kwargs (like head_mask) on purpose \n",
                "        # because ViTMAE layers typically reject them.\n",
                "        outputs = self.original_block(hidden_states)\n",
                "        \n",
                "        # 2. Extract the Hidden States\n",
                "        # Hugging Face models can return:\n",
                "        # - A tuple: (hidden_states, attention_weights, ...)\n",
                "        # - A ModelOutput object (like BaseModelOutput)\n",
                "        # - A raw Tensor\n",
                "        if isinstance(outputs, tuple):\n",
                "            x = outputs[0]\n",
                "        elif hasattr(outputs, \"hidden_states\"):\n",
                "            x = outputs.hidden_states\n",
                "        else:\n",
                "            x = outputs\n",
                "        \n",
                "        # 3. Apply the Adapter\n",
                "        # The adapter modifies the features in-place (conceptually) via residual connection.\n",
                "        x = self.adapter(x)\n",
                "        \n",
                "        # 4. Repackage Result\n",
                "        # We must return exactly what the parent model expects to avoid breaking the pipeline.\n",
                "        if isinstance(outputs, tuple):\n",
                "            # Reconstruct the tuple: (new_hidden_states, *rest_of_tuple)\n",
                "            return (x,) + outputs[1:]\n",
                "        elif hasattr(outputs, \"hidden_states\"):\n",
                "            # If it's a ModelOutput object, we try to update it.\n",
                "            # Some objects are immutable or downstream layers check strict types.\n",
                "            try:\n",
                "                outputs.hidden_states = x\n",
                "                return outputs\n",
                "            except:\n",
                "                # Fallback: Return a tuple, which HF pipelines usually accept as a valid alternative.\n",
                "                return (x,) \n",
                "        else:\n",
                "            # If input was just a Tensor, return the new Tensor.\n",
                "            return x\n",
                "\n",
                "\n",
                "def inject_adapters(model: PreTrainedModel, bottleneck_dim: int = 64) -> PreTrainedModel:\n",
                "    \"\"\"\n",
                "    Core Utility: Injects IBA Adapters into the Encoder of a Pre-trained Model.\n",
                "\n",
                "    This function performs the precise surgery needed to convert a standard \n",
                "    pre-trained model (like ViTMAE) into an adapter-tuned model.\n",
                "\n",
                "    Procedure\n",
                "    ---------\n",
                "    1.  **Freeze Backbone**: Sets `requires_grad=False` for ALL original parameters.\n",
                "    2.  **Locate Encoder**: Identifies the list of transformer layers (`encoder.layer`).\n",
                "    3.  **Inject Adapters**:\n",
                "        -   Iterates through each layer.\n",
                "        -   Creates a new `IBA_Adapter` matching the layer's dimensions.\n",
                "        -   Wraps the original layer in `ViTBlockWithAdapter`.\n",
                "        -   Replaces the layer in the model's module list.\n",
                "    4.  **Activate Adapters**: Ensures only the new adapter parameters are trainable.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    model : PreTrainedModel\n",
                "        The Hugging Face model instance (e.g., `ViTMAEForPreTraining`).\n",
                "    bottleneck_dim : int, optional\n",
                "        The dimension of the adapter bottleneck. Defaults to 64.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    PreTrainedModel\n",
                "        The modified model instance with adapters injected and backbone frozen.\n",
                "\n",
                "    Raises\n",
                "    ------\n",
                "    AttributeError\n",
                "        If the model structure is not recognized (i.e., cannot find the encoder layers).\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"[System] Starting Adapter Injection Procedure\")\n",
                "    print(f\"{'='*60}\")\n",
                "\n",
                "    # 1. Freeze the entire model backbone\n",
                "    # This ensures we don't destroy the pre-trained knowledge during fine-tuning.\n",
                "    print(\"[Config] Freezing original backbone parameters...\")\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "        \n",
                "    # 2. Locate the Encoder Module\n",
                "    # We inspect the model structure to find where the Transformer layers live.\n",
                "    if hasattr(model, \"vit\") and hasattr(model.vit, \"encoder\"):\n",
                "        # Standard ViTMAE structure (Hugging Face)\n",
                "        encoder = model.vit.encoder\n",
                "        config = model.config\n",
                "    elif hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
                "        # Generic BERT/ViT structure fallback\n",
                "        encoder = model.encoder\n",
                "        config = model.config\n",
                "    else:\n",
                "        raise AttributeError(\n",
                "            \"Could not locate 'encoder.layer'. \"\n",
                "            \"Model structure unknown (expected 'vit.encoder' or 'encoder').\"\n",
                "        )\n",
                "\n",
                "    input_dim = config.hidden_size\n",
                "    num_layers = len(encoder.layer)\n",
                "\n",
                "    print(f\"[Config] Model Config: Hidden Dim={input_dim}, Layers={num_layers}\")\n",
                "    print(f\"[Config] Adapter Config: Bottleneck Dim={bottleneck_dim}\")\n",
                "\n",
                "    # 3. Iterate and Replace\n",
                "    print(\"[Action] Injecting adapters into encoder layers...\")\n",
                "    \n",
                "    for i, layer in enumerate(encoder.layer):\n",
                "        # Create the adapter instance\n",
                "        adapter = IBA_Adapter(input_dim=input_dim, bottleneck_dim=bottleneck_dim)\n",
                "        \n",
                "        # CRITICAL: Move adapter to the correct device/dtype.\n",
                "        # This handles cases where the model is already on GPU or in FP16/BF16.\n",
                "        # We take the first parameter of the layer as a reference.\n",
                "        ref_param = next(layer.parameters())\n",
                "        adapter.to(device=ref_param.device, dtype=ref_param.dtype)\n",
                "        \n",
                "        # Wrap the original layer with our adapter-enabled wrapper\n",
                "        wrapped_layer = ViTBlockWithAdapter(original_block=layer, adapter=adapter)\n",
                "        \n",
                "        # Perform the replacement in the ModuleList\n",
                "        encoder.layer[i] = wrapped_layer\n",
                "        \n",
                "        # Progress logging\n",
                "        if (i + 1) % 4 == 0 or (i + 1) == num_layers:\n",
                "            print(f\"  -> Processed layer {i + 1}/{num_layers}\")\n",
                "\n",
                "    print(f\"[System] Injection Complete. Decoder layers ignored (if present).\")\n",
                "    \n",
                "    # 4. Verification\n",
                "    # Print a summary of trainable vs frozen parameters to confirm success.\n",
                "    count_trainable_params(model)\n",
                "    \n",
                "    return model\n",
                "\n",
                "\n",
                "def count_trainable_params(model: nn.Module) -> None:\n",
                "    \"\"\"\n",
                "    Audit Utility: Prints the distribution of Frozen vs Trainable parameters.\n",
                "    \n",
                "    Useful for verifying that:\n",
                "    1.  The backbone is indeed frozen (0 gradients).\n",
                "    2.  The adapters are trainable (requires_grad=True).\n",
                "    \n",
                "    Parameters\n",
                "    ----------\n",
                "    model : nn.Module\n",
                "        The model to inspect.\n",
                "    \"\"\"\n",
                "    total_params = sum(p.numel() for p in model.parameters())\n",
                "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "    frozen_params = total_params - trainable_params\n",
                "    \n",
                "    ratio = (trainable_params / total_params) * 100 if total_params > 0 else 0\n",
                "    \n",
                "    print(f\"\\n[Stats] Parameter Audit:\")\n",
                "    print(f\"  - Total Parameters:     {total_params:,}\")\n",
                "    print(f\"  - Frozen Backbone:      {frozen_params:,}\")\n",
                "    print(f\"  - Trainable (Adapters): {trainable_params:,}\")\n",
                "    print(f\"  - Trainable Ratio:      {ratio:.2f}%\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Main Execution Block (Integration Test)\n",
                "# =============================================================================\n",
                "if __name__ == \"__main__\":\n",
                "    \"\"\"\n",
                "    Test Script to verify the Adapter Injection pipeline.\n",
                "    \n",
                "    Steps:\n",
                "    1.  Load a real ViTMAE model from Hugging Face.\n",
                "    2.  Inject Adapters.\n",
                "    3.  Run a dummy forward pass to check for shape mismatches or runtime errors.\n",
                "    \"\"\"\n",
                "    print(\"[Main] Loading pre-trained ViTMAE...\")\n",
                "    try:\n",
                "        # NOTE: Requires `pip install transformers`\n",
                "        model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
                "        \n",
                "        # Inject Adapters\n",
                "        model = inject_adapters(model, bottleneck_dim=64)\n",
                "        \n",
                "        # Sanity Check: Forward pass\n",
                "        print(\"[Main] Running dummy forward pass to verify graph integrity...\")\n",
                "        dummy_input = torch.randn(1, 3, 224, 224)\n",
                "        \n",
                "        # Move inputs to same device as model\n",
                "        device = next(model.parameters()).device\n",
                "        dummy_input = dummy_input.to(device)\n",
                "        \n",
                "        # Forward pass (ensure gradients flow through adapters)\n",
                "        output = model(dummy_input)\n",
                "        \n",
                "        loss_val = output.loss.item() if hasattr(output, \"loss\") else \"N/A\"\n",
                "        print(f\"[Success] Standard Forward pass complete. Loss: {loss_val}\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"[Error] An error occurred during execution: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "97a6896e",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([1, 768])"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.vit(dummy_input).last_hidden_state.mean(dim=1).shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ffae6ad",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[[-0.1229,  0.0684,  0.1409,  ..., -0.1243, -0.1670, -0.0609],\n",
                            "         [-0.3416, -0.0233,  0.0788,  ..., -0.0027, -0.3302, -0.1642],\n",
                            "         [-0.1514, -0.0708,  0.2829,  ..., -0.1047, -0.2944, -0.5037],\n",
                            "         ...,\n",
                            "         [-0.4514, -0.0598,  0.3576,  ..., -0.0868, -0.2820, -0.7138],\n",
                            "         [ 0.5742,  0.0094, -0.0291,  ...,  0.0154, -0.3651,  0.0233],\n",
                            "         [ 0.4702, -0.0291,  0.1678,  ..., -0.0907, -0.2683, -0.6697]]],\n",
                            "       grad_fn=<NativeLayerNormBackward0>)"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.vit(dummy_input).last_hidden_state.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "f03415ea",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ViTMAEEncoder(\n",
                            "  (layer): ModuleList(\n",
                            "    (0-11): 12 x ViTBlockWithAdapter(\n",
                            "      (original_block): ViTMAELayer(\n",
                            "        (attention): ViTMAEAttention(\n",
                            "          (attention): ViTMAESelfAttention(\n",
                            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
                            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
                            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
                            "          )\n",
                            "          (output): ViTMAESelfOutput(\n",
                            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
                            "            (dropout): Dropout(p=0.0, inplace=False)\n",
                            "          )\n",
                            "        )\n",
                            "        (intermediate): ViTMAEIntermediate(\n",
                            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
                            "          (intermediate_act_fn): GELUActivation()\n",
                            "        )\n",
                            "        (output): ViTMAEOutput(\n",
                            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
                            "          (dropout): Dropout(p=0.0, inplace=False)\n",
                            "        )\n",
                            "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
                            "      )\n",
                            "      (adapter): IBA_Adapter(in_features=768, bottleneck=64)\n",
                            "    )\n",
                            "  )\n",
                            ")"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.vit.encoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3fc38d87",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from __future__ import annotations\n",
                "import copy\n",
                "import logging\n",
                "from typing import List, Dict, Any, Optional\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
                ")\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "\n",
                "class FederatedClient:\n",
                "    \"\"\"\n",
                "    Simulates a Local Client in the Federated Network.\n",
                "\n",
                "    Each client possesses a private local dataset (or a slice of the global data) \n",
                "    and trains a copy of the global model. \n",
                "\n",
                "    Key Responsibilities:\n",
                "    1.  **Phased Training**:\n",
                "        -   **Round 1**: Learns purely via Masked Autoencoding (MAE), establishing \n",
                "            a strong self-supervised baseline.\n",
                "        -   **Round > 1**: Learns via MAE + GPAD. It uses global prototypes received \n",
                "            from the server to regularize its feature space, preventing it from \n",
                "            drifting too far from the global consensus (Continual Learning).\n",
                "\n",
                "    2.  **Prototype Generation**:\n",
                "        -   After training, it extracts latent features from its data.\n",
                "        -   Runs an internal K-Means clustering algorithm to identify 'Local Prototypes'\n",
                "            representing the distinct visual concepts found in its private data.\n",
                "        -   Sends these prototypes (vectors) to the server, preserving data privacy \n",
                "            (raw images are never shared).\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self, \n",
                "        client_id: int,\n",
                "        model: nn.Module,\n",
                "        device: torch.device,\n",
                "        dtype: torch.dtype,\n",
                "        optimizer_cls: type = optim.AdamW,\n",
                "        optimizer_kwargs: Dict[str, Any] = None,\n",
                "        local_update_threshold: float = 0.7,\n",
                "        local_ema_alpha: float = 0.1,\n",
                "    ) -> None:\n",
                "        \"\"\"\n",
                "        Initialize the Client.\n",
                "\n",
                "        Args:\n",
                "            client_id (int): Unique identifier.\n",
                "            model (nn.Module): The base model architecture (ViT-MAE). \n",
                "                            Ideally, this is a deep copy of the global model.\n",
                "            device (torch.device): The hardware device (CPU/GPU) this client runs on.\n",
                "            optimizer_cls (type): The class of optimizer to use (default: AdamW).\n",
                "            optimizer_kwargs (Dict): Configuration for the local optimizer (lr, weight_decay).\n",
                "        \"\"\"\n",
                "        self.client_id = client_id\n",
                "        self.device = device\n",
                "        self.local_update_threshold = local_update_threshold\n",
                "        self.local_ema_alpha = local_ema_alpha\n",
                "        self.dtype = dtype\n",
                "        \n",
                "        # Local Prototypes State (Initialized as empty, populated/updated during lifecycle)\n",
                "        self.local_prototypes: Optional[torch.Tensor] = None\n",
                "        \n",
                "        # 1. Independent Model Copy\n",
                "        # We deepcopy the base model so that this client's training \n",
                "        # doesn't affect the base model or other clients.\n",
                "        self.model = copy.deepcopy(model).to(self.device)\n",
                "        \n",
                "        # 2. Independent Optimizer\n",
                "        opt_kwargs = optimizer_kwargs or {\"lr\": 1e-3}\n",
                "        self.optimizer = optimizer_cls(self.model.parameters(), **opt_kwargs)\n",
                "        \n",
                "        logger.info(f\"Client {self.client_id} initialized on {self.device}\")\n",
                "\n",
                "    def train_epoch(\n",
                "        self, \n",
                "        dataloader: DataLoader, \n",
                "        global_prototypes: torch.Tensor = None,\n",
                "        gpad_loss_fn: nn.Module = None\n",
                "    ) -> float:\n",
                "        \"\"\"\n",
                "        Executes one epoch of local training.\n",
                "\n",
                "        The loss function changes based on the availability of global prototypes:\n",
                "        -   **Initialization Phase (No Prototypes)**: Loss = L_mae\n",
                "        -   **Continual Phase (Has Prototypes)**: Loss = L_mae + L_gpad\n",
                "\n",
                "        Args:\n",
                "            dataloader (DataLoader): Local data stream.\n",
                "            global_prototypes (Tensor, optional): Global concepts from Server.\n",
                "            gpad_loss_fn (nn.Module, optional): The distillation loss module.\n",
                "\n",
                "        Returns:\n",
                "            float: Average loss across the epoch.\n",
                "        \"\"\"\n",
                "        self.model.train()\n",
                "        total_loss = 0.0\n",
                "        num_batches = 0\n",
                "        \n",
                "        for batch in dataloader:\n",
                "            inputs = batch.to(self.dtype).to(self.device)\n",
                "\n",
                "            # Forward Pass\n",
                "            outputs = self.model(inputs)\n",
                "\n",
                "            # 1. Base MAE Loss\n",
                "            mae_loss = getattr(outputs, \"loss\", None)\n",
                "            if mae_loss is None:\n",
                "                # Fallback if model doesn't compute loss internally (unlikely for ViTMAEForPreTraining)\n",
                "                mae_loss = torch.tensor(0.0, dtype=self.dtype, device=self.device, requires_grad=True)\n",
                "\n",
                "            final_loss = mae_loss\n",
                "\n",
                "            # --- Feature Extraction (Shared for GPAD and Local Proto Update) ---\n",
                "            embeddings = self.model.vit(dummy_input).last_hidden_state.mean(dim=1)\n",
                "        \n",
                "            # 2. GPAD Loss (if applicable)\n",
                "            if global_prototypes is not None and gpad_loss_fn is not None and embeddings is not None:\n",
                "                # Compute GPAD\n",
                "                # Ensure global prototypes are on same device\n",
                "                protos_device = global_prototypes.to(self.device)\n",
                "                gpad = gpad_loss_fn(embeddings, protos_device)\n",
                "                \n",
                "                final_loss = final_loss + gpad\n",
                "\n",
                "            # 3. Online Local Prototype Update (Separate Logic)\n",
                "            # \"Check sim with rest of the local prototypes -> Find Best -> EMA if > Threshold\"\n",
                "            if self.local_prototypes is not None and embeddings is not None:\n",
                "                if self.local_prototypes.device != self.device:\n",
                "                    self.local_prototypes = self.local_prototypes.to(self.device)\n",
                "\n",
                "\n",
                "                with torch.inference_mode():\n",
                "                    # Normalize for Cosine Similarity\n",
                "                    z_norm = F.normalize(embeddings, p=2, dim=1)\n",
                "                    p_norm = F.normalize(self.local_prototypes, p=2, dim=1)\n",
                "                    \n",
                "                    # Compute Similarity Matrix: (B, K_local)\n",
                "                    sims = torch.mm(z_norm, p_norm.t())\n",
                "                    \n",
                "                    # Find Best Matching Prototype per sample\n",
                "                    max_sim, best_idx = sims.max(dim=1)\n",
                "                    \n",
                "                    # Mask: Who passes the fixed threshold?\n",
                "                    mask = max_sim > self.local_update_threshold\n",
                "                    \n",
                "                    # Update Loop for matching samples\n",
                "                    indices = torch.where(mask)[0]\n",
                "                    if len(indices) > 0:\n",
                "                        for idx in indices:\n",
                "                            sample_emb = z_norm[idx]\n",
                "                            proto_idx = best_idx[idx]\n",
                "                            \n",
                "                            # EMA Update: Old = (1-a)Old + a*New\n",
                "                            old_proto = self.local_prototypes[proto_idx]\n",
                "                            updated_proto = (1 - self.local_ema_alpha) * old_proto + self.local_ema_alpha * sample_emb\n",
                "                            # In-place update\n",
                "                            self.local_prototypes[proto_idx] = updated_proto\n",
                "\n",
                "            # Backward Pass\n",
                "            self.optimizer.zero_grad()\n",
                "            final_loss.backward()\n",
                "            self.optimizer.step()\n",
                "\n",
                "            total_loss += final_loss.item()\n",
                "            num_batches += 1\n",
                "            \n",
                "        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
                "        return avg_loss\n",
                "\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def generate_prototypes(self, dataloader: DataLoader, K_init: int = 10) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Generates 'Local Prototypes' by clustering the client's feature space.\n",
                "\n",
                "        Process:\n",
                "        1.  Inference: Run the local model on all local data to extract embeddings.\n",
                "        2.  Clustering: Perform K-Means on these embeddings to find K centroids.\n",
                "        3.  These centroids become the 'Local Prototypes' sent to the server.\n",
                "\n",
                "        Args:\n",
                "            dataloader (DataLoader): Local data.\n",
                "            K_init (int): Number of prototypes to generate.\n",
                "\n",
                "        Returns:\n",
                "            torch.Tensor: Local prototypes [K, Dim].\n",
                "        \"\"\"\n",
                "        self.model.eval()\n",
                "        all_features = []\n",
                "        \n",
                "        # 1. Feature Extraction (Forward Pass)\n",
                "        for batch in dataloader:\n",
                "            inputs = batch.to(self.dtype).to(self.device)\n",
                "            \n",
                "            # Extract features from the model\n",
                "            # Assuming model returns an object with 'hidden_states' or similar, \n",
                "            # or for ViTMAE, we might need to tap into the encoder output.\n",
                "            # For simplicity, let's assume the model returns a direct embedding or 'last_hidden_state'\n",
                "            # If standard ViTMAE, outputs.last_hidden_state is (B, L, D). We usually pool it (e.g. CLS or mean).\n",
                "            # Let's assume Mean Pooling for prototype generation if sequence provided.\n",
                "            \n",
                "            with torch.inference_mode():\n",
                "                features = self.model.vit(inputs).last_hidden_state.mean(dim=1)\n",
                "                \n",
                "            all_features.append(features)\n",
                "\n",
                "        # Concatenate all features: (N_samples, D)\n",
                "        embeddings = torch.cat(all_features, dim=0)\n",
                "        \n",
                "        # 2. Normalization\n",
                "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
                "        \n",
                "        # 3. K-Means Clustering (Simple PyTorch Implementation)\n",
                "        centroids = self._kmeans(embeddings, K=K_init)\n",
                "        \n",
                "        # Save for next round's online updates\n",
                "        self.local_prototypes = centroids.detach().clone()\n",
                "        \n",
                "        return centroids\n",
                "\n",
                "    def _kmeans(self, X: torch.Tensor, K: int, max_iters: int = 100) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Simple K-Means implementation in PyTorch.\n",
                "        \"\"\"\n",
                "        N, D = X.shape\n",
                "        \n",
                "        # Initialize centroids randomly from data\n",
                "        indices = torch.randperm(N)[:K]\n",
                "        centroids = X[indices].clone()\n",
                "        \n",
                "        for _ in range(max_iters):\n",
                "            # Compute distances: ||x - c||^2\n",
                "            # (Using cosine distance since inputs are normalized)\n",
                "            # dist = 1 - cos_sim\n",
                "            \n",
                "            # Normalize centroids to keep consistent with embedding space (unit sphere)\n",
                "            centroids = torch.nn.functional.normalize(centroids, p=2, dim=1)\n",
                "            \n",
                "            # Similarity matrix: (N, K)\n",
                "            sims = torch.mm(X, centroids.t())\n",
                "            # Distance is monotonic with 1-sim, so maximizing sim is minimizing dist\n",
                "            \n",
                "            # Assign clusters\n",
                "            _, labels = sims.max(dim=1)\n",
                "            \n",
                "            # Update centroids\n",
                "            new_centroids = torch.zeros_like(centroids)\n",
                "            for k in range(K):\n",
                "                cluster_mask = (labels == k)\n",
                "                if cluster_mask.sum() > 0:\n",
                "                    new_centroids[k] = X[cluster_mask].mean(dim=0)\n",
                "                else:\n",
                "                    # Re-initialize empty cluster\n",
                "                    new_idx = torch.randint(0, N, (1,)).item()\n",
                "                    new_centroids[k] = X[new_idx]\n",
                "            \n",
                "            # Check convergence\n",
                "            center_shift = torch.norm(new_centroids - centroids)\n",
                "            centroids = new_centroids\n",
                "            if center_shift < 1e-4:\n",
                "                break\n",
                "                \n",
                "        return torch.nn.functional.normalize(centroids, p=2, dim=1)\n",
                "\n",
                "\n",
                "class ClientManager:\n",
                "    \"\"\"\n",
                "    Simulates the Orchestration of Multiple Clients.\n",
                "    \n",
                "    In a real FL system, this would be distributed across devices. Here, it manages\n",
                "    a list of `FederatedClient` objects and orchestrates their training, effectively\n",
                "    simulating the \"edge\" layer.\n",
                "\n",
                "    Execution Modes:\n",
                "    ----------------\n",
                "    1.  **Parallel (GPU)**: If GPUs are available (`gpu_count > 0`), it enforces\n",
                "        a strict 1:1 mapping (Client i -> GPU i) and runs training in parallel threads.\n",
                "    \n",
                "    2.  **Sequential (CPU)**: If no GPUs are available, it runs clients one after \n",
                "        another to avoid the overhead of threading on a single CPU resource.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self, \n",
                "        base_model: nn.Module, \n",
                "        num_clients: int, \n",
                "        gpu_count: int = 0\n",
                "    ) -> None:\n",
                "        \"\"\"\n",
                "        Initializes the Client Manager and spawns the clients.\n",
                "\n",
                "        Args:\n",
                "            base_model: The initial global model template.\n",
                "            num_clients: Total number of clients to simulate.\n",
                "            gpu_count: Number of available GPUs.\n",
                "        \"\"\"\n",
                "        self.clients: List[FederatedClient] = []\n",
                "        self.num_clients = num_clients\n",
                "        self.gpu_count = gpu_count\n",
                "        \n",
                "        self._initialize_clients(base_model)\n",
                "\n",
                "    def _initialize_clients(self, base_model: nn.Module) -> None:\n",
                "        \"\"\"Internal helper to spawn clients on appropriate devices.\"\"\"\n",
                "        logger.info(f\"Initializing {self.num_clients} clients...\")\n",
                "        \n",
                "        # Enforce 1:1 Mapping rule if GPUs are available\n",
                "        if self.gpu_count > 0:\n",
                "            if self.num_clients != self.gpu_count:\n",
                "                raise ValueError(\n",
                "                    f\"Strict 1:1 Client-GPU mapping required. \"\n",
                "                    f\"Requested {self.num_clients} clients but found {self.gpu_count} GPUs.\"\n",
                "                )\n",
                "            logger.info(f\"Parallel Mode: Mapped {self.num_clients} clients to {self.gpu_count} GPUs.\")\n",
                "        else:\n",
                "            logger.info(f\"Sequential Mode: Running {self.num_clients} clients on CPU.\")\n",
                "\n",
                "        for i in range(self.num_clients):\n",
                "            # Determine Device\n",
                "            if self.gpu_count > 0:\n",
                "                # 1:1 Mapping: Client i -> GPU i\n",
                "                device = torch.device(f\"cuda:{i}\")\n",
                "            else:\n",
                "                device = torch.device(\"cpu\")\n",
                "            \n",
                "            # Create Client\n",
                "            # Note: The optimizer config can be parameterized later\n",
                "            client = FederatedClient(\n",
                "                client_id=i,\n",
                "                model=base_model,\n",
                "                device=device,\n",
                "                optimizer_kwargs={\"lr\": 1e-4, \"weight_decay\": 0.05}\n",
                "            )\n",
                "            self.clients.append(client)\n",
                "\n",
                "    def train_round(\n",
                "        self, \n",
                "        dataloaders: List[DataLoader],\n",
                "        global_prototypes: torch.Tensor = None,\n",
                "        gpad_loss_fn: nn.Module = None\n",
                "    ) -> List[float]:\n",
                "        \"\"\"\n",
                "        Triggers one round of local training for ALL clients.\n",
                "        \n",
                "        Dispatch Logic:\n",
                "        -   **GPU Available**: Uses `ThreadPoolExecutor` to launch `N` concurrent threads.\n",
                "            Since PyTorch releases the GIL for CUDA operations, this achieves true parallelism.\n",
                "        -   **CPU Only**: Iterates sequentially. Python threads + CPU compute usually \n",
                "            degrades performance due to GIL contention, so sequential is faster here.\n",
                "        \n",
                "        Args:\n",
                "            dataloaders: List of DataLoaders (must match num_clients).\n",
                "            global_prototypes: The current global prototype bank (for GPAD).\n",
                "            gpad_loss_fn: The loss function instance.\n",
                "\n",
                "        Returns:\n",
                "            List[float]: The average training loss for each client.\n",
                "        \"\"\"\n",
                "        if len(dataloaders) != self.num_clients:\n",
                "            raise ValueError(\n",
                "                f\"Dataloader count ({len(dataloaders)}) does not match \"\n",
                "                f\"client count ({self.num_clients})\"\n",
                "            )\n",
                "\n",
                "        round_losses = [0.0] * self.num_clients\n",
                "        \n",
                "        if self.gpu_count > 0:\n",
                "            # Parallel Execution for GPUs\n",
                "            from concurrent.futures import ThreadPoolExecutor\n",
                "            \n",
                "            with ThreadPoolExecutor(max_workers=self.num_clients) as executor:\n",
                "                logger.info(f\"Spawning {self.num_clients} training threads (1 per GPU)...\")\n",
                "                futures = {}\n",
                "                for i, client in enumerate(self.clients):\n",
                "                    futures[executor.submit(\n",
                "                        client.train_epoch, \n",
                "                        dataloaders[i], \n",
                "                        global_prototypes=global_prototypes, \n",
                "                        gpad_loss_fn=gpad_loss_fn\n",
                "                    )] = i\n",
                "                \n",
                "                for future in futures:\n",
                "                    client_idx = futures[future]\n",
                "                    try:\n",
                "                        loss = future.result()\n",
                "                        round_losses[client_idx] = loss\n",
                "                        logger.info(f\"Client {client_idx} (GPU {client_idx}) finished. Loss: {loss:.4f}\")\n",
                "                    except Exception as e:\n",
                "                        logger.error(f\"Client {client_idx} failed: {e}\")\n",
                "                        round_losses[client_idx] = float('nan')\n",
                "        else:\n",
                "            # Sequential Execution for CPU\n",
                "            logger.info(f\"Running sequential training on CPU for {self.num_clients} clients...\")\n",
                "            for i, client in enumerate(self.clients):\n",
                "                try:\n",
                "                    loss = client.train_epoch(\n",
                "                        dataloaders[i],\n",
                "                        global_prototypes=global_prototypes,\n",
                "                        gpad_loss_fn=gpad_loss_fn\n",
                "                    )\n",
                "                    round_losses[i] = loss\n",
                "                    logger.info(f\"Client {i} (CPU) finished. Loss: {loss:.4f}\")\n",
                "                except Exception as e:\n",
                "                    logger.error(f\"Client {i} failed: {e}\")\n",
                "                    round_losses[i] = float('nan')\n",
                "            \n",
                "        return round_losses"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

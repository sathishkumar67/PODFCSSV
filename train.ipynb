{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1aa6f3d6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cloning into 'PODFCSSV'...\n",
                        "remote: Enumerating objects: 261, done.\u001b[K\n",
                        "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
                        "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
                        "remote: Total 261 (delta 0), reused 2 (delta 0), pack-reused 258 (from 1)\u001b[K\n",
                        "Receiving objects: 100% (261/261), 4.00 MiB | 18.97 MiB/s, done.\n",
                        "Resolving deltas: 100% (112/112), done.\n"
                    ]
                }
            ],
            "source": [
                "# import os\n",
                "# import shutil\n",
                "\n",
                "# for item in os.listdir('.'):\n",
                "#     if os.path.isfile(item) or os.path.islink(item):\n",
                "#         os.remove(item)\n",
                "#     elif os.path.isdir(item):\n",
                "#         shutil.rmtree(item)\n",
                "!git clone https://github.com/sathishkumar67/PODFCSSV.git\n",
                "!mv PODFCSSV/* /kaggle/working"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8e2fcd8d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Modules imported successfully.\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# IMPORT MODULES\n",
                "# =============================================================================\n",
                "import os\n",
                "import sys\n",
                "import logging\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from torchvision import transforms, datasets\n",
                "\n",
                "# Ensure local modules are found\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "# Explicit Imports from Project\n",
                "from src.client import ClientManager, FederatedClient\n",
                "from src.loss import GPADLoss\n",
                "from src.server import GlobalPrototypeBank, FederatedModelServer, run_server_round, GlobalModel\n",
                "\n",
                "# Configure Logging\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
                "logger = logging.getLogger(\"NotebookRequest\")\n",
                "\n",
                "print(\"Modules imported successfully.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "9204a8e2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-16 11:18:56,984 [INFO] NotebookRequest: Starting Pipeline Test with Config: {'num_clients': 2, 'num_rounds': 3, 'batch_size': 8, 'embedding_dim': 768, 'gpu_count': 2, 'merge_threshold': 0.85, 'ema_alpha': 0.1, 'gpad_base_tau': 0.5, 'gpad_temp_gate': 0.1, 'k_init_prototypes': 5}\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# CONFIGURATION\n",
                "# =============================================================================\n",
                "CONFIG = {\n",
                "    \"num_clients\": 2,          \n",
                "    \"num_rounds\": 3,           \n",
                "    \"batch_size\": 8,           \n",
                "    \"embedding_dim\": 768,      # ViT Base dim\n",
                "    \"gpu_count\": 2 if torch.cuda.device_count() >= 2 else torch.cuda.device_count(),\n",
                "    \n",
                "    # Loss & Proto Params\n",
                "    \"merge_threshold\": 0.85,\n",
                "    \"ema_alpha\": 0.1,\n",
                "    \"gpad_base_tau\": 0.5,\n",
                "    \"gpad_temp_gate\": 0.1,\n",
                "    \"k_init_prototypes\": 5,\n",
                "}\n",
                "\n",
                "logger.info(f\"Starting Pipeline Test with Config: {CONFIG}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "da0b6079",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-16 11:18:57,446 [INFO] NotebookRequest: Loading CIFAR10...\n",
                        "100%|██████████| 170M/170M [00:01<00:00, 104MB/s]  \n",
                        "2026-02-16 11:19:01,511 [INFO] NotebookRequest: Data Ready: 2 Clients with 50 samples each.\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# DATA PREPARATION (CIFAR10)\n",
                "# =============================================================================\n",
                "\n",
                "# Define Transforms (Resize to 224 for ViT)\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)), \n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "])\n",
                "\n",
                "# Load Dataset\n",
                "try:\n",
                "    logger.info(\"Loading CIFAR10...\")\n",
                "    full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
                "    \n",
                "    # Use a small subset for speed\n",
                "    subset_size = 100\n",
                "    indices = np.random.choice(len(full_dataset), subset_size, replace=False)\n",
                "    subset = torch.utils.data.Subset(full_dataset, indices)\n",
                "    \n",
                "    # Split for 2 clients\n",
                "    lengths = [subset_size // 2, subset_size - (subset_size // 2)]\n",
                "    ds1, ds2 = torch.utils.data.random_split(subset, lengths)\n",
                "    \n",
                "    dl1 = DataLoader(ds1, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
                "    dl2 = DataLoader(ds2, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
                "    dataloaders = [dl1, dl2]\n",
                "    \n",
                "    logger.info(f\"Data Ready: 2 Clients with {len(ds1)} samples each.\")\n",
                "    \n",
                "except Exception as e:\n",
                "    logger.error(f\"Data loading failed: {e}\")\n",
                "    raise\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "8d393892",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Main] Loading pre-trained ViTMAE...\n",
                        "\n",
                        "============================================================\n",
                        "[System] Starting Adapter Injection Procedure\n",
                        "============================================================\n",
                        "[Config] Freezing original backbone parameters...\n",
                        "[Config] Model Config: Hidden Dim=768, Layers=12\n",
                        "[Config] Adapter Config: Bottleneck Dim=64\n",
                        "[Action] Injecting adapters into encoder layers...\n",
                        "  -> Processed layer 4/12\n",
                        "  -> Processed layer 8/12\n",
                        "  -> Processed layer 12/12\n",
                        "[System] Injection Complete. Decoder layers ignored (if present).\n",
                        "\n",
                        "[Stats] Parameter Audit:\n",
                        "  - Total Parameters:     113,097,472\n",
                        "  - Frozen Backbone:      111,907,840\n",
                        "  - Trainable (Adapters): 1,189,632\n",
                        "  - Trainable Ratio:      1.05%\n",
                        "============================================================\n",
                        "\n",
                        "[Main] Running dummy forward pass to verify graph integrity...\n",
                        "[Success] Forward pass complete. Loss: 1.000892162322998\n"
                    ]
                }
            ],
            "source": [
                "from __future__ import annotations\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from typing import Optional, Tuple, Union, List, Any\n",
                "from transformers import PreTrainedModel, ViTMAEForPreTraining, ViTMAEModel\n",
                "\n",
                "\n",
                "\n",
                "class IBA_Adapter(nn.Module):\n",
                "    \"\"\"\n",
                "    Information-Bottlenecked Adapter (IBA) module.\n",
                "\n",
                "    This module implements a bottleneck architecture (Down-project -> Activation -> Up-project)\n",
                "    inserted into frozen networks to introduce trainable parameters for efficient adaptation.\n",
                "    \n",
                "    Architecture:\n",
                "        Input [B, L, D] -> Linear(D, d) -> Activation -> Linear(d, D) -> Dropout -> + Residual\n",
                "    \n",
                "    Key Design Principles:\n",
                "        1. **Bottleneck**: Compresses information to force the model to learn efficient features.\n",
                "        2. **Identity Initialization**: The up-projection is initialized to zero, ensuring \n",
                "        the adapter starts as an identity function (Adapter(x) = 0). This prevents \n",
                "        \"semantic shock\" to the pre-trained backbone at the start of training.\n",
                "\n",
                "    Attributes:\n",
                "        input_dim (int): Original hidden dimension.\n",
                "        bottleneck_dim (int): Compressed dimension.\n",
                "        down_project (nn.Linear): Dimensionality reduction layer.\n",
                "        activation (nn.Module): Non-linear activation function.\n",
                "        up_project (nn.Linear): Dimensionality restoration layer.\n",
                "        dropout (nn.Dropout): Regularization layer.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self, \n",
                "        input_dim: int, \n",
                "        bottleneck_dim: int = 64, \n",
                "        dropout: float = 0.0,\n",
                "        activation: nn.Module = nn.GELU()\n",
                "    ) -> None:\n",
                "        \"\"\"\n",
                "        Initializes the IBA Adapter.\n",
                "\n",
                "        Args:\n",
                "            input_dim (int): The hidden dimension of the backbone model (e.g., 768 for ViT-Base).\n",
                "            bottleneck_dim (int): The reduced dimension for the bottleneck. Lower values \n",
                "                compress information more (Information Bottleneck principle). Defaults to 64.\n",
                "            dropout (float): Dropout probability applied after the up-projection. Defaults to 0.0.\n",
                "            activation (nn.Module): Activation function to use between projections. Defaults to GELU.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.input_dim = input_dim\n",
                "        self.bottleneck_dim = bottleneck_dim\n",
                "        self.activation = activation\n",
                "\n",
                "        # Down-projection: Compress semantic information\n",
                "        self.down_project = nn.Linear(input_dim, bottleneck_dim)\n",
                "        \n",
                "        # Up-projection: Reconstruct features for the next layer\n",
                "        self.up_project = nn.Linear(bottleneck_dim, input_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        self._init_weights()\n",
                "\n",
                "    def _init_weights(self) -> None:\n",
                "        \"\"\"\n",
                "        Applies specific initialization strategies to ensure stable training start.\n",
                "        \"\"\"\n",
                "        # 1. Kaiming Normal for down_project to maintain variance through the non-linearity.\n",
                "        nn.init.kaiming_normal_(self.down_project.weight, nonlinearity='linear')\n",
                "        \n",
                "        # 2. Zeros for up_project. This ensures the adapter output is initially 0.\n",
                "        #    result = Input + 0. This preserves the pre-trained behavior exactly.\n",
                "        nn.init.zeros_(self.up_project.weight)\n",
                "        if self.up_project.bias is not None:\n",
                "            nn.init.zeros_(self.up_project.bias)\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass of the adapter.\n",
                "\n",
                "        Args:\n",
                "            x (torch.Tensor): Input tensor of shape [Batch_Size, Seq_Len, Hidden_Dim].\n",
                "\n",
                "        Returns:\n",
                "            torch.Tensor: Adapted features of the same shape as input.\n",
                "        \"\"\"\n",
                "        residual = x\n",
                "        \n",
                "        # Bottleneck compression\n",
                "        x = self.down_project(x)\n",
                "        x = self.activation(x)\n",
                "        \n",
                "        # Note: Variational noise injection (e.g., for Zeus/V4 methods) \n",
                "        # would typically be applied here if probabilistic modeling is desired.\n",
                "        \n",
                "        # Reconstruction & Regularization\n",
                "        x = self.up_project(x)\n",
                "        x = self.dropout(x)\n",
                "        \n",
                "        # Residual connection preserves original features while adding adaptation\n",
                "        return residual + x\n",
                "\n",
                "    def __repr__(self) -> str:\n",
                "        \"\"\"Custom string representation for easier debugging.\"\"\"\n",
                "        return f\"IBA_Adapter(in={self.input_dim}, btl={self.bottleneck_dim})\"\n",
                "\n",
                "\n",
                "class ViTBlockWithAdapter(nn.Module):\n",
                "    \"\"\"\n",
                "    Wrapper class to inject an Adapter into a Hugging Face ViTLayer.\n",
                "\n",
                "    It intercepts the output of the original frozen block, passes the hidden states\n",
                "    through the adapter, and repackages the output to match Hugging Face's \n",
                "    return signature exactly.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, original_block: nn.Module, adapter: IBA_Adapter) -> None:\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            original_block (nn.Module): The original, frozen Transformer block.\n",
                "            adapter (IBA_Adapter): The trainable adapter instance.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.original_block = original_block\n",
                "        self.adapter = adapter\n",
                "\n",
                "    def forward(\n",
                "        self, \n",
                "        hidden_states: torch.Tensor, \n",
                "        head_mask: Optional[torch.Tensor] = None, \n",
                "        output_attentions: bool = False,\n",
                "        **kwargs: Any\n",
                "    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, Any]]:\n",
                "        \"\"\"\n",
                "        Forward pass matching standard Hugging Face ViTLayer signature.\n",
                "        \n",
                "        Args:\n",
                "            hidden_states (torch.Tensor): Input tensor.\n",
                "            head_mask (Optional[torch.Tensor]): Mask for attention heads.\n",
                "            output_attentions (bool): Whether to return attention weights.\n",
                "            **kwargs: Additional arguments required by specific HF implementations.\n",
                "\n",
                "        Returns:\n",
                "            Tuple containing the modified hidden state and optional attention weights.\n",
                "        \"\"\"\n",
                "        # 1. Run the original frozen ViT Block\n",
                "        # HF blocks typically return a tuple: (hidden_states, attention_weights (optional), ...)\n",
                "        # We explicitly EXCLUDE output_attentions from the call as ViTMAE doesn't support it by default\n",
                "        outputs = self.original_block(\n",
                "            hidden_states, \n",
                "            head_mask=head_mask, \n",
                "            **kwargs\n",
                "        )\n",
                "        \n",
                "        # 2. Extract Hidden States and Logic for Return Packaging\n",
                "        if isinstance(outputs, tuple):\n",
                "            x = outputs[0]\n",
                "        elif hasattr(outputs, \"hidden_states\"):\n",
                "            x = outputs.hidden_states\n",
                "        else:\n",
                "            x = outputs\n",
                "        \n",
                "        # 3. Apply the IBA Adapter\n",
                "        x = self.adapter(x)\n",
                "        \n",
                "        # 4. Repackage output to maintain compatibility with HF pipeline\n",
                "        if isinstance(outputs, tuple):\n",
                "            # Reconstruct the tuple with the adapted hidden state\n",
                "            return (x,) + outputs[1:]\n",
                "        elif hasattr(outputs, \"hidden_states\"):\n",
                "            # If it's a ModelOutput, we try to create a new one or modify in place?\n",
                "            # Creating a new one is safer but requires knowing the class.\n",
                "            # Mutating in place works if it's mutable.\n",
                "            # A simpler hack that often works for HF is returning a tuple if it came as ModelOutput,\n",
                "            # but some downstream layers check isinstance(ModelOutput).\n",
                "            # However, standard ViTEncoder loop handles tuple or ModelOutput.\n",
                "            # But if it wasn't a tuple originally, let's try to return what it expects.\n",
                "            # Most robust: Just update the hidden_states attribute if mutable.\n",
                "            try:\n",
                "                outputs.hidden_states = x\n",
                "                return outputs\n",
                "            except:\n",
                "                # If immutable, we fallback to tuple which HF usually accepts\n",
                "                return (x,) \n",
                "        else:\n",
                "            # It was a Tensor, return a Tensor\n",
                "            return x\n",
                "\n",
                "\n",
                "def inject_adapters(model: PreTrainedModel, bottleneck_dim: int = 64) -> PreTrainedModel:\n",
                "    \"\"\"\n",
                "    Injects IBA Adapters into the Encoder of a ViTMAE (or similar) model.\n",
                "\n",
                "    This function performs the following operations:\n",
                "    1. Freezes all existing parameters in the model.\n",
                "    2. Identifies the Encoder layers.\n",
                "    3. Wraps each layer with `ViTBlockWithAdapter`.\n",
                "    4. Unfreezes ONLY the new Adapter parameters.\n",
                "\n",
                "    Args:\n",
                "        model (PreTrainedModel): The Hugging Face ViTMAE model instance.\n",
                "        bottleneck_dim (int): Dimension of the adapter bottleneck.\n",
                "\n",
                "    Returns:\n",
                "        PreTrainedModel: The modified model with adapters injected.\n",
                "    \n",
                "    Raises:\n",
                "        AttributeError: If the model structure does not match standard ViT hierarchies.\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"[System] Starting Adapter Injection Procedure\")\n",
                "    print(f\"{'='*60}\")\n",
                "\n",
                "    # 1. Freeze the entire model backbone\n",
                "    print(\"[Config] Freezing original backbone parameters...\")\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "        \n",
                "    # 2. Locate the Encoder\n",
                "    # We verify structure to prevent runtime errors later\n",
                "    if hasattr(model, \"vit\") and hasattr(model.vit, \"encoder\"):\n",
                "        # Standard ViTMAE structure\n",
                "        encoder = model.vit.encoder\n",
                "        config = model.config\n",
                "    elif hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):\n",
                "        # Generic BERT/ViT structure fallback\n",
                "        encoder = model.encoder\n",
                "        config = model.config\n",
                "    else:\n",
                "        raise AttributeError(\n",
                "            \"Could not locate 'encoder.layer'. \"\n",
                "            \"Model structure unknown (expected 'vit.encoder' or 'encoder').\"\n",
                "        )\n",
                "\n",
                "    input_dim = config.hidden_size\n",
                "    num_layers = len(encoder.layer)\n",
                "\n",
                "    print(f\"[Config] Model Config: Hidden Dim={input_dim}, Layers={num_layers}\")\n",
                "    print(f\"[Config] Adapter Config: Bottleneck Dim={bottleneck_dim}\")\n",
                "\n",
                "    # 3. Iterate and Replace\n",
                "    print(\"[Action] Injecting adapters into encoder layers...\")\n",
                "    \n",
                "    for i, layer in enumerate(encoder.layer):\n",
                "        # Instantiate the adapter\n",
                "        adapter = IBA_Adapter(input_dim=input_dim, bottleneck_dim=bottleneck_dim)\n",
                "        \n",
                "        # CRITICAL: Ensure adapter is on the same device and dtype as the layer it wraps.\n",
                "        # This handles cases where the model is already on GPU or in FP16/BF16.\n",
                "        ref_param = next(layer.parameters())\n",
                "        adapter.to(device=ref_param.device, dtype=ref_param.dtype)\n",
                "        \n",
                "        # Wrap the original layer\n",
                "        wrapped_layer = ViTBlockWithAdapter(original_block=layer, adapter=adapter)\n",
                "        \n",
                "        # Mutate the ModuleList in-place\n",
                "        encoder.layer[i] = wrapped_layer\n",
                "        \n",
                "        # Simple progress indicator for large models\n",
                "        if (i + 1) % 4 == 0 or (i + 1) == num_layers:\n",
                "            print(f\"  -> Processed layer {i + 1}/{num_layers}\")\n",
                "\n",
                "    print(f\"[System] Injection Complete. Decoder layers ignored (if present).\")\n",
                "    \n",
                "    # 4. Verification of Trainable Parameters\n",
                "    count_trainable_params(model)\n",
                "    \n",
                "    return model\n",
                "\n",
                "\n",
                "def count_trainable_params(model: nn.Module) -> None:\n",
                "    \"\"\"\n",
                "    Utility to calculate and print the count of frozen vs trainable parameters.\n",
                "    \n",
                "    Args:\n",
                "        model (nn.Module): The model to audit.\n",
                "    \"\"\"\n",
                "    total_params = sum(p.numel() for p in model.parameters())\n",
                "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "    frozen_params = total_params - trainable_params\n",
                "    \n",
                "    ratio = (trainable_params / total_params) * 100 if total_params > 0 else 0\n",
                "    \n",
                "    print(f\"\\n[Stats] Parameter Audit:\")\n",
                "    print(f\"  - Total Parameters:     {total_params:,}\")\n",
                "    print(f\"  - Frozen Backbone:      {frozen_params:,}\")\n",
                "    print(f\"  - Trainable (Adapters): {trainable_params:,}\")\n",
                "    print(f\"  - Trainable Ratio:      {ratio:.2f}%\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "\n",
                "\n",
                "# =============================================================================\n",
                "# Main Execution Block (For Testing)\n",
                "# =============================================================================\n",
                "if __name__ == \"__main__\":\n",
                "    # Simulate loading a model (mocking correct behavior if transformers is installed)\n",
                "    print(\"[Main] Loading pre-trained ViTMAE...\")\n",
                "    try:\n",
                "        # NOTE: Requires `pip install transformers`\n",
                "        model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
                "        \n",
                "        # Inject Adapters\n",
                "        model = inject_adapters(model, bottleneck_dim=64)\n",
                "        \n",
                "        # Sanity Check: Forward pass\n",
                "        print(\"[Main] Running dummy forward pass to verify graph integrity...\")\n",
                "        dummy_input = torch.randn(1, 3, 224, 224)\n",
                "        \n",
                "        # Move inputs to same device as model\n",
                "        device = next(model.parameters()).device\n",
                "        dummy_input = dummy_input.to(device)\n",
                "        \n",
                "        # Forward pass (ensure gradients flow through adapters)\n",
                "        output = model(dummy_input)\n",
                "        \n",
                "        loss_val = output.loss.item() if hasattr(output, \"loss\") else \"N/A\"\n",
                "        print(f\"[Success] Forward pass complete. Loss: {loss_val}\")\n",
                "        \n",
                "    except ImportError:\n",
                "        print(\"[Error] 'transformers' library not found. Please install it to run this test.\")\n",
                "    except Exception as e:\n",
                "        print(f\"[Error] An error occurred during execution: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "989e59ab",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-16 11:20:09,628 [INFO] NotebookRequest: Initializing ViT-MAE Backbone...\n",
                        "2026-02-16 11:20:09,935 [INFO] NotebookRequest: Adapters Injected Successfully.\n",
                        "2026-02-16 11:20:09,935 [INFO] src.client: Initializing 2 clients...\n",
                        "2026-02-16 11:20:09,936 [INFO] src.client: Parallel Mode: Mapped 2 clients to 2 GPUs.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "[System] Starting Adapter Injection Procedure\n",
                        "============================================================\n",
                        "[Config] Freezing original backbone parameters...\n",
                        "[Config] Model Config: Hidden Dim=768, Layers=12\n",
                        "[Config] Adapter Config: Bottleneck Dim=64\n",
                        "[Action] Injecting adapters into encoder layers...\n",
                        "  -> Processed layer 4/12\n",
                        "  -> Processed layer 8/12\n",
                        "  -> Processed layer 12/12\n",
                        "[System] Injection Complete. Decoder layers ignored (if present).\n",
                        "\n",
                        "[Stats] Parameter Audit:\n",
                        "  - Total Parameters:     113,097,472\n",
                        "  - Frozen Backbone:      111,907,840\n",
                        "  - Trainable (Adapters): 1,189,632\n",
                        "  - Trainable Ratio:      1.05%\n",
                        "============================================================\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-16 11:20:10,278 [INFO] src.client: Client 0 initialized on cuda:0\n",
                        "2026-02-16 11:20:10,552 [INFO] src.client: Client 1 initialized on cuda:1\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# INITIALIZE COMPONENTS\n",
                "# =============================================================================\n",
                "\n",
                "# 1. Global Prototype Bank\n",
                "proto_bank = GlobalPrototypeBank(\n",
                "    embedding_dim=CONFIG[\"embedding_dim\"],\n",
                "    merge_threshold=CONFIG[\"merge_threshold\"],\n",
                "    ema_alpha=CONFIG[\"ema_alpha\"],\n",
                "    device=\"cpu\" \n",
                ")\n",
                "\n",
                "# 2. Global Model Server (Aggregation)\n",
                "fed_server = FederatedModelServer()\n",
                "\n",
                "# 3. Base Model (ViT-MAE with Adapters)\n",
                "try:\n",
                "    from transformers import ViTMAEForPreTraining\n",
                "    \n",
                "    logger.info(\"Initializing ViT-MAE Backbone...\")\n",
                "    base_model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
                "    base_model = inject_adapters(base_model, bottleneck_dim=64)\n",
                "    logger.info(\"Adapters Injected Successfully.\")\n",
                "    \n",
                "except ImportError:\n",
                "    logger.warning(\"Transformers not found. Logic will fail unless mocked.\")\n",
                "    raise\n",
                "\n",
                "# 4. Client Manager\n",
                "client_manager = ClientManager(\n",
                "    base_model=base_model,\n",
                "    num_clients=CONFIG[\"num_clients\"],\n",
                "    gpu_count=CONFIG[\"gpu_count\"]\n",
                ")\n",
                "\n",
                "# 5. GPAD Loss\n",
                "gpad_loss = GPADLoss(\n",
                "    base_tau=CONFIG[\"gpad_base_tau\"],\n",
                "    temp_gate=CONFIG[\"gpad_temp_gate\"]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "801d75ee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-16 11:20:18,223 [INFO] NotebookRequest: \n",
                        "--- Starting Round 1 / 3 ---\n",
                        "2026-02-16 11:20:18,224 [INFO] NotebookRequest: > Clients Training...\n",
                        "2026-02-16 11:20:18,224 [INFO] src.client: Spawning 2 training threads (1 per GPU)...\n",
                        "2026-02-16 11:20:19,565 [INFO] src.client: Client 0 (GPU 0) finished. Loss: 0.0140\n",
                        "2026-02-16 11:20:19,566 [INFO] src.client: Client 1 (GPU 1) finished. Loss: 0.0136\n",
                        "2026-02-16 11:20:19,567 [INFO] NotebookRequest:   Mean Batch Loss per Client: [0.013959189078637533, 0.013591428553419454]\n",
                        "2026-02-16 11:20:19,568 [INFO] NotebookRequest: > Extracting Prototypes and Weights...\n",
                        "2026-02-16 11:20:21,024 [INFO] NotebookRequest: > Server Aggregating...\n",
                        "2026-02-16 11:20:21,405 [INFO] NotebookRequest:   Round Complete. Global Prototype Bank Size: 3\n",
                        "2026-02-16 11:20:21,406 [INFO] NotebookRequest: \n",
                        "--- Starting Round 2 / 3 ---\n",
                        "2026-02-16 11:20:21,407 [INFO] NotebookRequest: > Broadcasting Global Weights...\n",
                        "2026-02-16 11:20:21,652 [INFO] NotebookRequest: > Clients Training...\n",
                        "2026-02-16 11:20:21,653 [INFO] src.client: Spawning 2 training threads (1 per GPU)...\n",
                        "2026-02-16 11:20:22,735 [INFO] src.client: Client 0 (GPU 0) finished. Loss: 0.3892\n",
                        "2026-02-16 11:20:22,736 [INFO] src.client: Client 1 (GPU 1) finished. Loss: 0.4038\n",
                        "2026-02-16 11:20:22,737 [INFO] NotebookRequest:   Mean Batch Loss per Client: [0.38915742720876423, 0.4037518799304962]\n",
                        "2026-02-16 11:20:22,738 [INFO] NotebookRequest: > Extracting Prototypes and Weights...\n",
                        "2026-02-16 11:20:23,963 [INFO] NotebookRequest: > Server Aggregating...\n",
                        "2026-02-16 11:20:24,259 [INFO] NotebookRequest:   Round Complete. Global Prototype Bank Size: 3\n",
                        "2026-02-16 11:20:24,260 [INFO] NotebookRequest: \n",
                        "--- Starting Round 3 / 3 ---\n",
                        "2026-02-16 11:20:24,260 [INFO] NotebookRequest: > Broadcasting Global Weights...\n",
                        "2026-02-16 11:20:24,507 [INFO] NotebookRequest: > Clients Training...\n",
                        "2026-02-16 11:20:24,508 [INFO] src.client: Spawning 2 training threads (1 per GPU)...\n",
                        "2026-02-16 11:20:25,306 [INFO] src.client: Client 0 (GPU 0) finished. Loss: 0.3347\n",
                        "2026-02-16 11:20:25,307 [INFO] src.client: Client 1 (GPU 1) finished. Loss: 0.3700\n",
                        "2026-02-16 11:20:25,308 [INFO] NotebookRequest:   Mean Batch Loss per Client: [0.33465673668043955, 0.37004748412540983]\n",
                        "2026-02-16 11:20:25,309 [INFO] NotebookRequest: > Extracting Prototypes and Weights...\n",
                        "2026-02-16 11:20:26,427 [INFO] NotebookRequest: > Server Aggregating...\n",
                        "2026-02-16 11:20:26,626 [INFO] NotebookRequest:   Round Complete. Global Prototype Bank Size: 3\n",
                        "2026-02-16 11:20:26,627 [INFO] NotebookRequest: \n",
                        "*** Pipeline Execution Finished Successfully ***\n"
                    ]
                }
            ],
            "source": [
                "# =============================================================================\n",
                "# EXECUTE PIPELINE\n",
                "# =============================================================================\n",
                "global_protos = None\n",
                "global_weights = None\n",
                "\n",
                "for round_idx in range(1, CONFIG[\"num_rounds\"] + 1):\n",
                "    logger.info(f\"\\n--- Starting Round {round_idx} / {CONFIG['num_rounds']} ---\")\n",
                "    \n",
                "    # A. Broadcast Global Weights (if exists)\n",
                "    if global_weights is not None:\n",
                "        logger.info(\"> Broadcasting Global Weights...\")\n",
                "        for client in client_manager.clients:\n",
                "            client.model.load_state_dict(global_weights, strict=False)\n",
                "            \n",
                "    # B. Client Training Step\n",
                "    logger.info(\"> Clients Training...\")\n",
                "    losses = client_manager.train_round(\n",
                "        dataloaders,\n",
                "        global_prototypes=global_protos,\n",
                "        gpad_loss_fn=gpad_loss\n",
                "    )\n",
                "    logger.info(f\"  Mean Batch Loss per Client: {losses}\")\n",
                "    \n",
                "    # C. Extract Payloads (Protos + Weights)\n",
                "    client_payloads = []\n",
                "    logger.info(\"> Extracting Prototypes and Weights...\")\n",
                "    for i, client in enumerate(client_manager.clients):\n",
                "        # Generate Local Prototypes (K-Means)\n",
                "        local_protos = client.generate_prototypes(dataloaders[i], K_init=CONFIG[\"k_init_prototypes\"])\n",
                "        \n",
                "        # Get Weights (CPU)\n",
                "        weights = {k: v.cpu() for k, v in client.model.state_dict().items()}\n",
                "        \n",
                "        client_payloads.append({\n",
                "            'client_id': f\"client_{i}\",\n",
                "            'protos': local_protos.cpu(),\n",
                "            'weights': weights\n",
                "        })\n",
                "        \n",
                "    # D. Server Aggregation\n",
                "    logger.info(\"> Server Aggregating...\")\n",
                "    server_result = run_server_round(\n",
                "        proto_manager=proto_bank,\n",
                "        model_server=fed_server,\n",
                "        client_payloads=client_payloads\n",
                "    )\n",
                "    \n",
                "    global_protos = server_result['global_prototypes']\n",
                "    global_weights = server_result['global_weights']\n",
                "    \n",
                "    proto_count = global_protos.shape[0] if global_protos is not None else 0\n",
                "    logger.info(f\"  Round Complete. Global Prototype Bank Size: {proto_count}\")\n",
                "\n",
                "logger.info(\"\\n*** Pipeline Execution Finished Successfully ***\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83851c81",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

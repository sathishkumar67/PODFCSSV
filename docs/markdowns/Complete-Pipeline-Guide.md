# COMPLETE FEDERATED CONTINUAL SELF-SUPERVISED VISION PIPELINE
## Federated Continual Self-Supervised Vision via Gated Prototype Anchored Distillation
### Full Implementation Guide with All Enhancements

---

## TABLE OF CONTENTS

1. Problem Formulation & Motivation
2. System Architecture Overview
3. Complete Step-by-Step Pipeline (0-8)
4. Enhanced Components (6 Enhancements)
5. Mathematical Formulations
6. Implementation Details
7. Experimental Setup
8. Evaluation Metrics
9. Timeline & Deliverables
10. Publication Strategy

---

# 1. PROBLEM FORMULATION & MOTIVATION

## The Challenge

You have a distributed federated learning environment where:
- **Multiple clients** have decentralized, unlabeled, non-IID vision data
- **Privacy constraints** prevent sharing raw data
- **Continual learning requirement**: New tasks arrive sequentially; models must learn without forgetting old tasks
- **Resource constraints**: Edge devices have limited communication and compute

Traditional solutions fail:
- âŒ Centralized learning: violates privacy
- âŒ Standard FedAvg: doesn't handle sequential tasks (catastrophic forgetting)
- âŒ Centralized continual learning: requires labeled data
- âŒ Individual client training: loses collaborative knowledge

## Your Solution

A novel framework combining:
1. **Self-Supervised Pretraining** (MAE): Learn from unlabeled data
2. **Federated Aggregation**: Share knowledge without exposing data
3. **Prototype-Based Memory**: Compact semantic representation
4. **Gated Distillation**: Adaptive knowledge transfer with confidence weighting
5. **Continual Learning**: Evolving prototypes across tasks

---

# 2. SYSTEM ARCHITECTURE OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FEDERATED CONTINUAL SELF-SUPERVISED VISION SYSTEM          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ LAYER 1: Self-Supervised Pretraining (SSL)          â”‚     â”‚
â”‚  â”‚ - Masked Image Modeling (MAE) on unlabeled data      â”‚     â”‚
â”‚  â”‚ - Federated aggregation of encoder weights           â”‚     â”‚
â”‚  â”‚ - Output: Learned visual backbone                    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ LAYER 2: Parameter-Efficient Fine-Tuning            â”‚     â”‚
â”‚  â”‚ - Vision Transformer (ViT) backbone (FROZEN)         â”‚     â”‚
â”‚  â”‚ - Task-specific adapters or LoRA modules             â”‚     â”‚
â”‚  â”‚ - Only 2-5% of parameters trainable                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ LAYER 3: Prototype-Based Knowledge Distillation      â”‚     â”‚
â”‚  â”‚ - Local prototype extraction (k-means clustering)     â”‚     â”‚
â”‚  â”‚ - Global prototype aggregation (weighted merging)     â”‚     â”‚
â”‚  â”‚ - Gated confidence-weighted distillation loss         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ LAYER 4: Continual Learning Memory                   â”‚     â”‚
â”‚  â”‚ - Experience replay with uncertainty sampling         â”‚     â”‚
â”‚  â”‚ - Novelty detection and dynamic prototype expansion   â”‚     â”‚
â”‚  â”‚ - Catastrophic forgetting prevention                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ LAYER 5: Evaluation & Analysis                       â”‚     â”‚
â”‚  â”‚ - Per-task accuracy tracking                         â”‚     â”‚
â”‚  â”‚ - Forgetting and backward transfer metrics           â”‚     â”‚
â”‚  â”‚ - Communication efficiency analysis                  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```python
```

## STEP 5: ROUND-2+ LOCAL TRAINING (ACTIVE MEMORY) (Per Client)

### 5.1 Forward Pass & Assignment

```python
def forward_with_prototype_assignment(client, batch_images, global_prototypes):
    """
    STEP 5.1-5.2: Forward Pass & Global Prototype Assignment
    
    For each embedding:
    1. Compute similarity to all global prototypes
    2. Assign to best match (or use soft assignment)
    """
    
    # Forward pass through adapted model
    logits, embeddings = client.model(batch_images, return_features=True)
    # embeddings: [B, 768]
    
    # Normalize embeddings
    embeddings_norm = F.normalize(embeddings, dim=1)
    global_protos_norm = F.normalize(global_prototypes, dim=1)
    
    # Compute similarity to all global prototypes
    similarity_scores = torch.mm(embeddings_norm, global_protos_norm.T)
    # [B, K_global]
    
    # Option A: Hard assignment (argmax)
    best_proto_idx = similarity_scores.argmax(dim=1)  # [B]
    best_similarity = similarity_scores.gather(1, best_proto_idx.unsqueeze(1)).squeeze()  # [B]
    
    # Option B: Soft assignment (recommended - prevents mode collapse)
    temperature = 0.07  # Temperature for softmax
    soft_weights = F.softmax(similarity_scores / temperature, dim=1)  # [B, K_global]
    
    return {
        'embeddings': embeddings,
        'logits': logits,
        'similarity_scores': similarity_scores,
        'best_proto_idx': best_proto_idx,
        'best_similarity': best_similarity,
        'soft_weights': soft_weights,
    }
```

### 5.2 Gated Prototype Distillation Loss

```python
class GatedPrototypeDistillationLoss(nn.Module):
    """
    STEP 5.3: Gated Prototype Distillation (THE KEY NOVELTY)
    
    Novel mechanism: Use adaptive, confidence-weighted gating to apply
    prototype distillation only to "confident" assignments, preventing
    learning from noisy or uncertain prototype matches.
    
    Two variants:
    A. Binary gating (simpler, less flexible)
    B. Soft gating (recommended, state-of-art)
    """
    
    def __init__(self, temperature=0.07, distillation_weight=0.5, 
                 gating_type='soft', tau_threshold=0.5):
        super().__init__()
        self.temperature = temperature
        self.distillation_weight = distillation_weight
        self.gating_type = gating_type
        self.tau_threshold = tau_threshold
    
    def forward(self, embeddings, best_similarity, global_prototypes, 
                prototype_confidence, global_counts, current_task):
        """
        Args:
            embeddings: [B, 768] - client embeddings
            best_similarity: [B] - cosine similarity to best prototype
            global_prototypes: [K, 768]
            prototype_confidence: [K] - confidence score per prototype
            global_counts: [K] - usage count per prototype
            current_task: int - current task ID
        
        Returns:
            L_proto: Prototype distillation loss
            gate_weights: [B] - gate weights per sample
        """
        
        embeddings_norm = F.normalize(embeddings, dim=1)
        global_protos_norm = F.normalize(global_prototypes, dim=1)
        
        # Recompute similarities (for loss)
        sims = torch.mm(embeddings_norm, global_protos_norm.T)  # [B, K]
        best_proto_idx = sims.argmax(dim=1)  # [B]
        
        # ENHANCEMENT 4: Adaptive Uncertainty-Based Gating
        # Compute uncertainty per sample
        entropy = -torch.sum(F.softmax(sims, dim=1) * 
                            torch.log_softmax(sims, dim=1) + 1e-8, dim=1)
        entropy_norm = entropy / np.log(len(global_prototypes))  # Normalize to [0,1]
        
        # Adaptive threshold: higher uncertainty â†’ stricter gating
        tau_adaptive = self.tau_threshold - 0.2 * entropy_norm  # [B]
        
        # ====================
        # GATING OPTIONS
        # ====================
        
        if self.gating_type == 'binary':
            # Option A: Binary gating (deprecated, too coarse)
            gate = (best_similarity >= self.tau_threshold).float()
        
        elif self.gating_type == 'soft_sigmoid':
            # Option B: Soft sigmoid gating (RECOMMENDED)
            # Smooth transition around threshold
            gate = torch.sigmoid((best_similarity - tau_adaptive) / self.temperature)
            # [B]
        
        elif self.gating_type == 'confidence_weighted':
            # Option C: Confidence-aware loss weighting
            # Weight loss by how confident we are in the prototype
            proto_conf = prototype_confidence[best_proto_idx]  # [B]
            gate = proto_conf * torch.sigmoid((best_similarity - tau_adaptive) / self.temperature)
            # [B]
        
        elif self.gating_type == 'soft_assignment':
            # Option D: Fully soft (use all prototypes, not just best)
            # Distance to best prototype
            soft_sims = F.softmax(sims / self.temperature, dim=1)  # [B, K]
            proto_loss = (soft_sims * (embeddings_norm.unsqueeze(1) - 
                                      global_protos_norm.unsqueeze(0)) ** 2).sum(dim=2)  # [B, K]
            proto_loss = proto_loss.mean(dim=1)  # [B]
            
            # Weight by average prototype confidence
            gate = F.softmax(soft_sims, dim=1) @ prototype_confidence  # [B]
            L_proto = (gate * proto_loss).mean()
            
            return L_proto, gate
        
        # Distillation loss: L2 distance to best prototype
        best_protos = global_protos_norm[best_proto_idx]  # [B, 768]
        L_proto_base = torch.norm(embeddings_norm - best_protos, p=2, dim=1)  # [B]
        
        # Apply gating
        L_proto = (gate * L_proto_base).mean()
        
        return L_proto, gate

class ClientTrainingRound2Plus:
    """
    STEP 5: Complete Round 2+ training with active memory
    """
    
    def train_continual_round(self, round_id, task_id, global_model, 
                             global_prototypes, global_counts, 
                             global_confidence):
        """
        Train on round â‰¥ 2 with prototype-guided distillation
        """
        
        # Update model from server
        self.model.load_state_dict(global_model)
        
        # Store global prototypes
        self.global_prototypes = global_prototypes
        self.global_confidence = global_confidence
        
        # Optimizer
        optimizer = torch.optim.Adam(
            [p for name, p in self.model.named_parameters() if 'adapter' in name],
            lr=0.001
        )
        
        # Loss functions
        ce_loss = nn.CrossEntropyLoss()
        proto_loss_fn = GatedPrototypeDistillationLoss(
            temperature=0.07,
            distillation_weight=0.5,
            gating_type='soft_sigmoid',  # Soft gating (RECOMMENDED)
            tau_threshold=0.5
        )
        
        # Curriculum learning: warm-up prototype loss weight
        if round_id < 5:
            lambda_proto = 0.1 * (round_id / 5.0)  # Gradually introduce
        else:
            lambda_proto = 1.0  # Full weight after warmup
        
        # Training loop
        num_epochs = self.config['local_epochs']
        
        for epoch in range(num_epochs):
            for batch_idx, (images, labels) in enumerate(self.local_loader):
                images, labels = images.to(self.device), labels.to(self.device)
                
                # Forward pass with prototype assignment
                results = forward_with_prototype_assignment(
                    self, images, global_prototypes
                )
                
                embeddings = results['embeddings']
                logits = results['logits']
                best_similarity = results['best_similarity']
                
                # Loss 1: Standard classification loss (if labels available)
                # In unsupervised case, can use pseudo-labels or skip
                L_ce = ce_loss(logits, labels)
                
                # Loss 2: Gated Prototype Distillation Loss (NOVEL)
                L_proto, gate_weights = proto_loss_fn(
                    embeddings, best_similarity, global_prototypes,
                    global_confidence, global_counts, task_id
                )
                
                # Combined loss with curriculum scheduling
                L_total = L_ce + lambda_proto * L_proto
                
                optimizer.zero_grad()
                L_total.backward()
                optimizer.step()
                
                # Logging
                if batch_idx % 50 == 0:
                    print(f"[Client {self.client_id}] Round {round_id} Epoch {epoch} "
                          f"L_ce: {L_ce.item():.4f} L_proto: {L_proto.item():.4f} "
                          f"gate: {gate_weights.mean().item():.3f}")
        
        return self.model.state_dict()
```

---

## STEP 6: UPDATE LOCAL PROTOTYPES (Per Client)

### 6.1 Prototype Evolution with Novelty Detection

```python
class LocalPrototypeManagerV2(LocalPrototypeManager):
    """
    STEP 6: Update Local Prototypes with Novelty Detection
    
    Features:
    - EMA updates for known prototypes
    - Novelty detection for unknown concepts
    - Dynamic prototype expansion when new concepts accumulate
    """
    
    def update_prototypes(self, new_embeddings, round_id, task_id,
                         global_prototypes, global_confidence):
        """
        Update local prototypes after training round
        
        Args:
            new_embeddings: [N, 768] - new embeddings from this round
            round_id: current round number
            task_id: current task ID
            global_prototypes: [K, 768] - global reference
            global_confidence: [K] - confidence of global prototypes
        """
        
        new_embeddings_norm = F.normalize(new_embeddings, dim=1)
        global_protos_norm = F.normalize(global_prototypes, dim=1)
        
        # Step 1: Assign new embeddings to existing prototypes
        sims = torch.mm(new_embeddings_norm, self.prototypes.T)  # [N, K_local]
        assigned_idx = sims.argmax(dim=1)  # [N]
        assigned_sims = sims.gather(1, assigned_idx.unsqueeze(1)).squeeze()  # [N]
        
        # Step 2: EMA update for assigned prototypes
        alpha = self.alpha_ema
        
        for k in range(len(self.prototypes)):
            mask = assigned_idx == k
            if mask.sum() > 0:
                z_mean = new_embeddings_norm[mask].mean(dim=0)
                # EMA update
                self.prototypes[k] = (1 - alpha) * self.prototypes[k] + alpha * z_mean
                self.prototypes[k] = F.normalize(self.prototypes[k], dim=0)
                self.prototype_counts[k] += mask.sum().item()
        
        # Step 3: Detect novelty
        # Samples with low similarity to all local prototypes
        novel_mask = assigned_sims < self.novelty_threshold
        
        if novel_mask.sum() > 0:
            novel_embeddings = new_embeddings_norm[novel_mask]
            self.novelty_buffer.extend(novel_embeddings.tolist())
            
            print(f"[Client {self.client_id}] Detected {novel_mask.sum()} novel samples")
        
        # Step 4: Create new prototypes from novelty buffer
        if len(self.novelty_buffer) > 50:
            # Cluster novelty buffer
            novel_tensor = torch.stack(
                [torch.tensor(e) for e in self.novelty_buffer]
            )
            
            # k-means on novelty buffer
            from sklearn.cluster import KMeans
            num_new_clusters = min(3, len(novel_tensor) // 10)
            
            if num_new_clusters > 0:
                kmeans = KMeans(n_clusters=num_new_clusters, n_init=5)
                labels = kmeans.fit_predict(novel_tensor.cpu().numpy())
                
                # Extract new prototypes
                new_protos = []
                for cluster_id in range(num_new_clusters):
                    cluster_mask = torch.tensor(labels) == cluster_id
                    cluster_proto = novel_tensor[cluster_mask].mean(dim=0)
                    cluster_proto = F.normalize(cluster_proto, dim=0)
                    new_protos.append(cluster_proto)
                
                # Append to local prototypes
                self.prototypes = torch.cat([
                    self.prototypes,
                    torch.stack(new_protos)
                ])
                
                # Initialize counts for new prototypes
                new_counts = torch.full((num_new_clusters,), 10.0)
                self.prototype_counts = torch.cat([
                    self.prototype_counts, new_counts
                ])
                
                self.novelty_buffer = []  # Clear buffer
                
                print(f"[Client {self.client_id}] Created {num_new_clusters} new prototypes")
        
        # Step 5: Periodic consolidation (remove low-usage prototypes)
        if round_id % 10 == 0:
            self.consolidate_prototypes()
    
    def consolidate_prototypes(self):
        """
        ENHANCEMENT 2: Server-side style consolidation at client
        
        Remove very similar prototypes to reduce redundancy
        """
        # Compute similarity matrix
        sims = torch.mm(self.prototypes, self.prototypes.T)
        
        # Find pairs with high similarity
        threshold = 0.95
        to_remove = set()
        
        for i in range(len(self.prototypes)):
            for j in range(i+1, len(self.prototypes)):
                if sims[i, j] > threshold and j not in to_remove:
                    # Merge j into i (keep the more frequently used)
                    if self.prototype_counts[j] > self.prototype_counts[i]:
                        to_remove.add(i)
                    else:
                        to_remove.add(j)
        
        # Keep only non-removed prototypes
        keep_mask = torch.tensor(
            [i not in to_remove for i in range(len(self.prototypes))]
        )
        
        if keep_mask.sum() < len(self.prototypes):
            self.prototypes = self.prototypes[keep_mask]
            self.prototype_counts = self.prototype_counts[keep_mask]
            
            print(f"[Client] Consolidated prototypes: "
                  f"{len(self.prototypes)} â†’ {keep_mask.sum()}")
```

---

## STEP 7: SEND UPDATES TO SERVER (Per Client)

### 7.1 Client Transmission

```python
def client_send_updates(client, round_id, task_id):
    """
    STEP 7: Client sends updates to server
    
    What to send:
    - Updated local prototypes
    - Prototype counts (sample contribution)
    - Prototype confidence scores
    - Prototype task tags
    - Adapter weights (updated)
    - Novelty flags (which prototypes are new)
    
    What NOT to send:
    - Full embeddings
    - Raw data
    - Encoder weights (frozen)
    
    Total size: ~510 KB per client per round
    """
    
    client_message = {
        'client_id': client.client_id,
        'round_id': round_id,
        'task_id': task_id,
        
        # Prototypes
        'local_prototypes': client.prototype_manager.prototypes,  # [K_local, 768]
        'prototype_counts': client.prototype_manager.prototype_counts,  # [K_local]
        'prototype_confidence': compute_client_confidence(client),  # [K_local]
        'prototype_task_id': torch.full((len(client.prototype_manager.prototypes),), 
                                       task_id, dtype=torch.long),
        
        # Model updates
        'adapter_params': {
            name: param.cpu().clone() for name, param in 
            client.model.named_parameters() if 'adapter' in name
        },
        
        # Metadata
        'data_size': len(client.replay_buffer),
        'novelty_count': len(client.prototype_manager.novelty_buffer),
    }
    
    return client_message

def compute_client_confidence(client):
    """Confidence score for each local prototype"""
    # Count-based + variance-based confidence
    total_count = client.prototype_manager.prototype_counts.sum()
    count_conf = client.prototype_manager.prototype_counts / total_count
    
    # Variance-based (placeholder - would need actual variance)
    variance_conf = torch.ones_like(count_conf)
    
    return count_conf * variance_conf
```

---

## STEP 8: SERVER MEMORY UPDATE (Server)

### 8.1 Prototype Matching & Merging

```python
def server_update_prototypes(server, client_messages, round_id, task_id):
    """
    STEP 8: Server Update Global Prototypes
    
    Process:
    1. Receive local prototypes from all clients
    2. Match each local â†’ best global prototype
    3. Merge if match is good, else create new
    4. Update confidence scores and metadata
    5. Optionally consolidate to prevent explosion
    """
    
    print(f"\n[Server] Round {round_id} Task {task_id} - Updating global prototypes")
    
    # Collect all client prototypes
    all_client_protos = []
    all_client_counts = []
    all_client_conf = []
    
    for msg in client_messages:
        all_client_protos.append(msg['local_prototypes'])
        all_client_counts.append(msg['prototype_counts'])
        all_client_conf.append(msg['prototype_confidence'])
    
    # ====================
    # MATCHING STRATEGY
    # ====================
    
    # OPTION A: Simple Cosine Matching (Fast, recommended for K < 100)
    
    similarity_threshold = 0.80
    
    for client_idx, client_protos in enumerate(all_client_protos):
        for local_idx, local_proto in enumerate(client_protos):
            local_norm = F.normalize(local_proto.unsqueeze(0), dim=1)
            global_norm = F.normalize(server.global_prototypes, dim=1)
            
            # Cosine similarity
            sims = torch.mm(local_norm, global_norm.T)[0]  # [K_global]
            best_match_idx = sims.argmax()
            best_sim = sims[best_match_idx]
            
            if best_sim > similarity_threshold:
                # Merge: weighted average
                count_local = all_client_counts[client_idx][local_idx]
                count_global = server.prototype_counts[best_match_idx]
                conf_local = all_client_conf[client_idx][local_idx]
                conf_global = server.prototype_confidence[best_match_idx]
                
                # Weight by confidence and count
                total_weight = count_local * conf_local + count_global * conf_global
                merged = (count_local * conf_local * local_proto +
                         count_global * conf_global * server.global_prototypes[best_match_idx]
                         ) / (total_weight + 1e-8)
                merged = F.normalize(merged, dim=0)
                
                server.global_prototypes[best_match_idx] = merged
                server.prototype_counts[best_match_idx] = (count_local + count_global) / 2
                server.prototype_last_used[best_match_idx] = round_id
                
            else:
                # Create new global prototype
                server.global_prototypes = torch.cat([
                    server.global_prototypes,
                    local_proto.unsqueeze(0)
                ])
                server.prototype_counts = torch.cat([
                    server.prototype_counts,
                    torch.tensor([all_client_counts[client_idx][local_idx]])
                ])
                server.prototype_confidence = torch.cat([
                    server.prototype_confidence,
                    all_client_conf[client_idx][local_idx].unsqueeze(0)
                ])
                server.prototype_created_round = torch.cat([
                    server.prototype_created_round,
                    torch.tensor([round_id])
                ])
                server.prototype_last_used = torch.cat([
                    server.prototype_last_used,
                    torch.tensor([round_id])
                ])
                server.task_per_prototype = torch.cat([
                    server.task_per_prototype,
                    torch.tensor([task_id])
                ])
    
    # ====================
    # CONSOLIDATION
    # ====================
    
    # ENHANCEMENT 2: Periodically consolidate to prevent explosion
    if round_id % 10 == 0:
        server.consolidate_prototypes()
    
    # ====================
    # UPDATE MODEL WEIGHTS
    # ====================
    
    # Aggregate adapter weights from all clients (FedAvg)
    aggregated_adapters = {}
    for param_name in client_messages[0]['adapter_params'].keys():
        param_values = [msg['adapter_params'][param_name] 
                       for msg in client_messages]
        aggregated = torch.stack(param_values).mean(dim=0)
        aggregated_adapters[param_name] = aggregated
    
    # Load into server model
    server.global_model.load_state_dict(aggregated_adapters, strict=False)
    
    print(f"[Server] Global prototypes: {len(server.global_prototypes)} "
          f"(added {len(all_client_protos)} new this round)")
    
    return server.global_prototypes, aggregated_adapters

def consolidate_prototypes(server):
    """
    ENHANCEMENT 2: Remove dead/redundant prototypes
    """
    # Remove unused prototypes
    min_usage_ratio = 0.01
    total_usage = server.prototype_counts.sum()
    
    active_mask = server.prototype_counts > (min_usage_ratio * total_usage)
    
    # Merge very similar ones
    sims = torch.mm(server.global_prototypes, server.global_prototypes.T)
    merge_threshold = 0.95
    
    for i in range(len(server.global_prototypes)):
        for j in range(i+1, len(server.global_prototypes)):
            if active_mask[i] and active_mask[j]:
                if sims[i, j] > merge_threshold:
                    # Merge j into i
                    total_count = server.prototype_counts[i] + server.prototype_counts[j]
                    merged = (server.prototype_counts[i] * server.global_prototypes[i] +
                             server.prototype_counts[j] * server.global_prototypes[j]
                             ) / total_count
                    server.global_prototypes[i] = F.normalize(merged, dim=0)
                    server.prototype_counts[i] = total_count
                    active_mask[j] = False
    
    # Keep only active
    if active_mask.sum() < len(server.global_prototypes):
        server.global_prototypes = server.global_prototypes[active_mask]
        server.prototype_counts = server.prototype_counts[active_mask]
        server.prototype_confidence = server.prototype_confidence[active_mask]
        server.prototype_created_round = server.prototype_created_round[active_mask]
        server.prototype_last_used = server.prototype_last_used[active_mask]
        server.task_per_prototype = server.task_per_prototype[active_mask]
        
        print(f"[Server] Consolidated prototypes: "
              f"{len(server.global_prototypes)} â†’ {active_mask.sum()}")
```

---

# 4. ENHANCED COMPONENTS (6 Enhancements Summary)

| # | Enhancement | Where Applied | Impact | Literature |
|---|-------------|----------------|--------|-----------|
| 1 | Prototype Confidence Scoring | STEP 2, 5.3 | Weight prototypes by quality | FedRFQ |
| 2 | Server Denoising | STEP 8 | Remove dead/redundant prototypes | FedRFQ |
| 3 | Task-Aware Tags | STEP 2-8 | Tag prototypes with task_id | V-LETO |
| 4 | Adaptive Uncertainty Gating | STEP 5.3 | Ï„ varies by embedding uncertainty | FedTA |
| 5 | Differential Privacy | STEP 7 | DP noise on prototypes | BAPFL |
| 6 | Optimal Transport Matching | STEP 8 | Hungarian algorithm for matching | FedAli |

---

# 5. MATHEMATICAL FORMULATIONS

## 5.1 Local Prototype Computation (STEP 1)

```
Given: Local embeddings Z = {z_1, ..., z_N} âˆˆ R^(NÃ—d)
Output: Local prototypes P = {Î¼_1, ..., Î¼_K}

1. Normalize: Z_norm = Z / ||Z||_2
2. K-means clustering: C_1, ..., C_K = KMeans(Z_norm, k=K)
3. Prototype computation: Î¼_k = mean(Z_norm[C_k])
4. Normalize: Î¼_k = Î¼_k / ||Î¼_k||_2
5. Store counts: n_k = |C_k|
```

## 5.2 Global Prototype Aggregation (STEP 2)

```
Given: Client prototypes {P^(i) : i=1..M}, {C^(i) : counts}
Output: Global prototypes G

Bootstrap (Round 1):
1. Concatenate: P_all = âˆª_i P^(i)
2. Similarity matrix: S[i,j] = cos(P_all[i], P_all[j])
3. Redundancy removal:
   For each prototype p:
     Find similar: {q : cos(p,q) > Ï„_merge}
     Merge: Î¼_merged = (Î£ n_q Â· q) / Î£ n_q
     Replace cluster with single prototype
4. Result: G = merged prototypes
```

## 5.3 Gated Prototype Distillation Loss (STEP 5.3)

```
Given: Embedding z, global prototypes G, similarity s = cos(z, G[k*])
       where k* = argmax_k cos(z, G[k])

Soft Gating Loss (RECOMMENDED):

1. Normalize: z_norm = z / ||z||_2

2. Compute confidence-weighted gate:
   gate = sigmoid((s - Ï„_adaptive) / T)
   where Ï„_adaptive = Ï„_base - Î»Â·H(z)
         H(z) = entropy of z over prototypes
         T = temperature (0.07)

3. Prototype loss:
   L_proto = gate Â· ||z_norm - G[k*]_norm||_2

4. Combined loss (curriculum scheduled):
   L_total = L_CE + Î»_proto(round) Â· L_proto
   where Î»_proto(round) = { 0.1Â·round/5 if round < 5
                           { 1.0         otherwise
```

## 5.4 Confidence Score Computation (ENHANCEMENT 1)

```
For each prototype Î¼_k:

Confidence_k = (count_k / Î£ count_i) Ã— (1 - var_k / max_var)

where:
  - count_k: number of samples contributing to Î¼_k
  - var_k: variance of samples around Î¼_k (estimated as distance std)
  - Normalized to [0, 1]

Use in gating:
  gate = proto_confidence[k*] Ã— sigmoid(...)
```

---

# 6. IMPLEMENTATION DETAILS

## 6.1 Configuration Template

```python
config = {
    # Model
    'vit_model': 'vit_base_patch16',
    'embedding_dim': 768,
    'num_adapters': 12,
    'adapter_hidden_dim': 64,
    
    # Data
    'num_clients': 10,
    'num_tasks': 5,
    'classes_per_task': 20,
    'dirichlet_alpha': 0.5,  # Non-IID parameter (lower = more non-IID)
    'batch_size': 32,
    
    # Prototypes
    'K': 20,  # Number of local prototypes per client
    'prototype_merge_threshold': 0.85,  # Redundancy removal threshold
    'alpha_ema': 0.1,  # EMA update rate
    'novelty_threshold': 0.4,  # Below this = novel
    'novelty_buffer_size': 500,
    
    # Training
    'local_epochs': 10,
    'total_rounds': 100,  # Per task
    'learning_rate': 0.001,
    'temperature': 0.07,
    
    # Losses
    'lambda_proto': 0.5,  # Prototype distillation weight (with curriculum)
    'gating_type': 'soft_sigmoid',  # soft_sigmoid, confidence_weighted, soft_assignment
    'tau_threshold': 0.5,
    
    # Communication
    'client_fraction': 0.8,  # Fraction of clients per round
    'use_compression': False,
    
    # Enhancements
    'enable_confidence_scoring': True,  # Enhancement 1
    'enable_denoising': True,  # Enhancement 2 (every 10 rounds)
    'enable_task_aware_tags': True,  # Enhancement 3
    'enable_adaptive_gating': True,  # Enhancement 4
    'enable_differential_privacy': False,  # Enhancement 5 (optional)
    'enable_optimal_transport': False,  # Enhancement 6 (optional, slower)
}
```

## 6.2 Key Hyperparameters

```
Critical Hyperparameters (tune these):

1. alpha_ema [0.05 - 0.2]
   - Recommendation: 0.1 (medium stability)
   - Lower: more stable but slow adaptation
   - Higher: faster adaptation, more noise

2. novelty_threshold [0.3 - 0.6]
   - Recommendation: 0.4
   - Affects how many samples are deemed "novel"
   - Too high: miss real novelty
   - Too low: false positives

3. prototype_merge_threshold [0.80 - 0.95]
   - Recommendation: 0.85
   - How similar before merging in STEP 2
   - Affects global prototype bank size

4. lambda_proto_schedule
   - Recommendation: 0.1 * round for first 5 rounds, then 1.0
   - Prevents prototype noise from early learning

5. temperature (gating) [0.05 - 0.15]
   - Recommendation: 0.07
   - Standard in contrastive learning
   - Controls softness of gating

6. tau_threshold [0.3 - 0.7]
   - Recommendation: 0.5
   - Adaptive gating: reduced by uncertainty
   - Sets confidence bar for prototype matching
```

---

# 7. EXPERIMENTAL SETUP

## 7.1 Datasets & Splits

```python
# Primary dataset: CIFAR-100
- 100 classes, 50K train, 10K test
- Image size: 32Ã—32 (resize to 224Ã—224 for ViT)
- Non-IID split: Dirichlet(Î±=0.5) across 10 clients
- Class-incremental: 5 tasks of 20 classes each

# Alternative: Tiny-ImageNet
- 200 classes, 100K train
- Image size: 64Ã—64
- More challenging, larger scale

# For testing domain shift:
- CIFAR-10 â†’ CIFAR-100 transition
- Synthetic noise at task boundaries
```

## 7.2 Evaluation Protocol

```python
# After each task:
1. Accuracy on all seen tasks
2. Forgetting measure: drop in old task accuracy
3. Backward transfer: improvement in old tasks from new learning
4. Forward transfer: improvement in new task from old knowledge

# Communication tracking:
1. Bytes sent per round (adapters + prototypes)
2. Compression ratio (if applicable)
3. Total communication cost vs baselines

# Computational cost:
1. Training time per round
2. Inference latency
3. Memory usage per client
```

## 7.3 Baseline Comparisons

```
1. FedAvg (standard federated averaging)
   - No prototypes, no continual handling
   - Baseline: expect 45-55% accuracy

2. FedAvg + Replay (experience replay only)
   - No prototypes, simple replay buffer
   - Expected: 55-65% accuracy

3. FedProto (prototype-based FL)
   - Class prototypes (needs labels)
   - Expected: 65-70% accuracy

4. FedGPD (global prototype distillation)
   - Supervised prototype distillation
   - Expected: 70-75% accuracy

5. FedAli (optimal transport alignment)
   - State-of-art prototype FL
   - Expected: 72-75% accuracy

6. V-LETO (vertical FCL with prototypes)
   - Closest to yours (but vertical FL)
   - Expected: 74-76% accuracy

7. Your Method (Full)
   - Expected: 75-78% accuracy
   - With all 6 enhancements: 78-82%
```

---

# 8. EVALUATION METRICS

## 8.1 Accuracy Metrics

```python
class AccuracyTracker:
    def __init__(self, num_tasks):
        self.task_accuracies = {}  # {task_id: [acc_after_task0, acc_after_task1, ...]}
        self.num_tasks = num_tasks
    
    def compute_average_accuracy(self):
        """Average accuracy across all tasks"""
        flat = [acc for accs in self.task_accuracies.values() for acc in accs]
        return np.mean(flat)
    
    def compute_forgetting(self):
        """Backward forgetting: how much did old tasks degrade"""
        forgetting_per_task = []
        for task_id in range(self.num_tasks - 1):  # Exclude last task
            # Accuracy on task_id immediately after learning
            acc_after = self.task_accuracies[task_id][task_id]
            # Best accuracy on task_id after all subsequent tasks
            acc_best_later = max(self.task_accuracies[j][task_id] 
                                for j in range(task_id + 1, self.num_tasks))
            forgetting = acc_after - acc_best_later
            forgetting_per_task.append(forgetting)
        
        return np.mean(forgetting_per_task)
    
    def compute_backward_transfer(self):
        """Does learning new tasks help old tasks?"""
        transfer = 0
        for task_id in range(self.num_tasks - 1):
            # Change in accuracy on old task after learning all subsequent tasks
            change = (self.task_accuracies[self.num_tasks-1][task_id] -
                     self.task_accuracies[task_id][task_id])
            transfer += change
        
        return transfer / (self.num_tasks - 1)
```

## 8.2 Communication Efficiency

```python
# Bytes transmitted per round per client:
# Adapters: 12 layers Ã— 64Ã—768 + 768Ã—64 = ~600 KB (LoRA: ~100 KB)
# Prototypes: 20 Ã— 768 Ã— 4 bytes = ~60 KB
# Total: ~660 KB per round

# Communication reduction vs baseline:
# FedAvg (full ViT): 350 MB
# Your method: 0.66 MB
# Reduction: ~500x or 0.2% of full model communication

# Track over all rounds:
total_bytes = sum_over_all_rounds(adapter_bytes + prototype_bytes)
reduction_factor = total_bytes / (full_model_bytes * num_rounds)
```

---

# 9. TIMELINE & DELIVERABLES

## Week-by-Week Schedule

```
Weeks 1-2: PHASE 0 - Environment & Data
 âœ“ Install PyTorch, timm, dependencies
 âœ“ Download CIFAR-100
 âœ“ Create non-IID data splits
 âœ“ Implement DataLoaders
 Deliverable: data_split.py, test on 1 client

Weeks 3-4: PHASE 1 & 2 - Local Training + Bootstrap
 âœ“ Implement MAE encoder + adapters
 âœ“ Implement k-means prototype extraction
 âœ“ Implement server bootstrap with redundancy removal
 Deliverable: working client + server loop for round 1

Weeks 5-6: PHASE 3 & 4 - Broadcast & Alignment
 âœ“ Implement prototype broadcasting
 âœ“ Implement passive alignment phase
 âœ“ Add logging & visualization
 Deliverable: test data flow for round 1 â†’ 2 transition

Weeks 7-8: PHASE 5 - Active Training
 âœ“ Implement gated prototype distillation loss
 âœ“ Implement soft gating (multiple variants)
 âœ“ Add curriculum learning for Î»_proto
 Deliverable: successful training on small experiment (2 clients, 2 tasks)

Weeks 9-10: PHASE 6, 7, 8 - Updates & Aggregation
 âœ“ Implement local prototype updates with EMA
 âœ“ Implement novelty detection
 âœ“ Implement server merging with optimal matching
 âœ“ Add Enhancement 1 (confidence scoring)
 âœ“ Add Enhancement 2 (denoising)
 Deliverable: complete pipeline end-to-end

Weeks 11-12: EXPERIMENTS
 âœ“ Run full experiment: 10 clients, 5 tasks, 100 rounds
 âœ“ Baseline comparisons (FedAvg, FedProto, FedGPD)
 âœ“ Ablation studies (each enhancement Â±)
 Deliverable: Results table, accuracy curves

Weeks 13-14: ENHANCEMENTS & ANALYSIS
 âœ“ Add Enhancement 3 (task-aware tags)
 âœ“ Add Enhancement 4 (adaptive gating)
 âœ“ Analyze communication cost
 âœ“ Non-IID robustness (vary Î±)
 Deliverable: comprehensive results, visualizations

Weeks 15-16: WRITING & FINALIZATION
 âœ“ Write method section (Steps 0-8)
 âœ“ Write experiments section
 âœ“ Create figures (pipeline, results, ablations)
 âœ“ Prepare for presentation
 Deliverable: complete research paper draft
```

---

# 10. PUBLICATION STRATEGY

## Paper Structure

```
1. Abstract (150 words)
   - Problem: FCL with privacy + heterogeneity
   - Solution: Gated prototype distillation
   - Results: +30% over FedAvg, -80% forgetting

2. Introduction
   - Federated learning challenges
   - Continual learning challenges
   - State-of-art (mention FedProto, FedGPD, V-LETO)
   - Your contribution: gated mechanism + MAE

3. Related Work
   - Federated Learning (FedAvg, FedProto, FedGPD)
   - Continual Learning (EWC, replay, prototypes)
   - Self-Supervised Learning (MAE, BEiT)
   - Combined approaches

4. Method
   - Problem formulation & motivation
   - Complete pipeline (Steps 0-8) with math
   - 6 enhancements (as extensions)
   - Loss functions and algorithms

5. Experiments
   - Dataset & setup
   - Baselines & ablations
   - Main results table
   - Non-IID robustness
   - Communication analysis

6. Results & Analysis
   - Accuracy trends
   - Forgetting reduction
   - Ablation study
   - Communication efficiency
   - Visualizations (t-SNE of prototypes)

7. Discussion
   - Why gated mechanism helps
   - Comparison to baselines
   - Limitations
   - Future work

8. Conclusion

9. References (40-50 papers)

10. Appendix
    - Detailed algorithms
    - Hyperparameter tuning
    - Additional results
    - Implementation details
```

## Key Differentiators for Reviewers

1. **Novel gated mechanism** - soft sigmoid/confidence weighting never done before
2. **Cluster prototypes** - no labels needed (unsupervised)
3. **Comprehensive ablations** - proves each component matters
4. **Strong baselines** - compare to FedProto, FedGPD, FedAli
5. **6 enhancements** - shows depth of investigation
6. **Communication analysis** - shows practical applicability

---

# SUMMARY

This complete pipeline represents a novel, publication-quality research contribution combining:

- **MAE-based self-supervised pretraining** (STEP 0-1)
- **Federated prototype aggregation** (STEP 2, 8)
- **Gated confidence-weighted distillation** (STEP 5.3) - YOUR KEY INNOVATION
- **Continual learning with dynamic prototypes** (STEP 6)
- **6 research enhancements** (confidence scoring, denoising, task-awareness, adaptive gating, DP, optimal transport)

**Expected Results:**
- Base method: 74-75% accuracy
- With all 6 enhancements: 78-82% accuracy
- Forgetting reduction: -80% vs FedAvg
- Communication efficiency: 500Ã— reduction

**Timeline:** 16 weeks (realistic for final-year project)

**Publication Potential:** IJCAI/AAAI main or top workshop with proper experiments and ablations.

---

This is your complete, industry-grade research project. Good luck! ðŸš€
